{
    "SIT796-1.1P.txt": "SIT796 Reinforcement Learning\nPass Task 1.1: Reinforcement Learning Environments\nOverview\nDuring week 1, you have learnt about Machine Learning, AI and Reinforcement Learning.\nYou have also been briefed about some basic reinforcement learning terminology along with\nexamples of learning environments.\nIn this task you, will conduct a case study about one of the gym environments from here:\nhttps://gymnasium.farama.org/ . Navigate to the \u2018Environments\u2019 section on the left side (see\nscreenshot below), and pick any 1 environment within any of the categories (Classic Control,\nBox2D etc.,).\n\nTo complete this assignment, you may refer to the lecture material from Week 1.\n\nSubmission Details\nSubmit a report (via OnTrack) discussing the following aspects for any chosen gym\nenvironment:\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\nDescribe the states, actions, transition functions and rewards for it.\nIs it a discrete or continuous state environment? Justify.\nIs it a discrete or continuous action environment? Justify.\nIs the transition function deterministic or probabilistic? Why?\nWhat would the optimal policy look like? Describe how the reward function enables\nthe learning of such an optimal policy.\n\nConstraints\nThe submitted report should not exceed 600 words in length. Please make sure the report\nis well-organised, and maintains a high quality of writing with appropriate references\nincluded if required.\n\n\f",
    "Week01_02_Introduction_to_Reinforcement_Learning.txt": "SIT796 Reinforcement Learning\nMachine and Reinforcement Learning in\nHistory\n\nPresented by:\nDr. Thommen George Karimpanal\nSchool of Information Technology\nDeakin University CRICOS Provider Code: 00113B\n\n\fRL\n\nSource: https://en.wikipedia.org/wiki/Reinforcement_learning\n\n\fRL Origins\n\nRichard Bellman\n(1950s)\n\nRichard Sutton and Andrew Barto\n(since 1980\u2019s)\n\n\fArtificial Intelligence Throughout\nHistory\nJacquard loom of early 1800s\n\u2022\n\nTranslated card patterns into cloth designs\n\nCharles Babbage\u2019s analytical engine\n(1830s & 40s)\n\u2022\n\nPrograms were cards with data and operations\n\nAda Lovelace \u2013 first programmer\n\n\u201cThe engine can arrange and combine its\nnumerical quantities exactly as if they were\nletters or any other general symbols; And in fact\nmight bring out its results in algebraic notation,\nwere provision made.\u201d\n\n\fThe First Computers\n1946- John Von\nNeumann\nled a team that built\ncomputers with stored\nprograms and a central\nprocessor\nENIAC, however, was\nalso programmed with\npatch cords.\nVon Neuman with ENIAC\n\n\fArtificial Intelligence Throughout\nHistory\n\u2022 1950 - Alan Turing\n\u2022\n\nPublishes \u201cComputing Machinery and Intelligence\n\n\u2022\n\nProposes \u201cthe imitation game\u201d which will later become known as the \u201cTuring\nTest.\u201d\n\n\u2022 1951- Marvin Minsky and Dean Edmunds\n\u2022\n\nBuild SNARC (Stochastic Neural Analog Reinforcement Calculator)\n\n\u2022\n\nThis is the first artificial neural network, using 3000 vacuum tubes to simulate\na network of 40 neurons.\n\n6\nDeakin University CRICOS Provider Code: 00113B\n\n\fArtificial Intelligence Throughout\nHistory\n\u2022 August 31, 1955 - John McCarthy, Marvin Minsky,\nNathaniel Rochester, and Claude Shannon\n\u2022\n\nThe term \u201cartificial intelligence\u201d is coined\n\n\u2022\n\nThey propose a \u201c2 month, 10 man study of artificial intelligence\u201d\n\n\u2022\n\nSubmitted to the workshop at Dartmouth College\n\n\u2022\n\nConsidered as the official birthdate of the new field\n\n\u2022 December 1955 - Herbert Simon and Allen Newell\n\n7\n\n\u2022\n\nThey develop the Logic Theorist, the first artificial intelligence program\n\n\u2022\n\n\u201cThe Theorist\u201d proved 38 of the first 52 theorems in Whitehead and Russell's\nPrincipia Mathematica.\n\nDeakin University CRICOS Provider Code: 00113B\n\n\fArtificial Intelligence Throughout\nHistory: Decision Making\n\nClassical control\ntheory\n\nAutonomous decision\nmaking:\n\nMDPs, Dynamic\nprogramming\n8\nDeakin University CRICOS Provider Code: 00113B\n\n\fArtificial Intelligence Throughout\nHistory\n\u2022 McCulloch & Pitts\n\n9\n\n\u2022\n\nPitts: self-taught genius, left home at 15\n\n\u2022\n\nMcCulloch was impressed by Pitts, homeless at\nthe time\n\n\u2022\n\nCollaborated to write the seminal paper: \"A\nLogical Calculus of Ideas Immanent in Nervous\nActivity\" -First mathematical model of a neural\nnetwork\n\n\u2022\n\nResearch findings that the brain was not solely\nresponsible for image processing\n\n\u2022\n\nBurned his unpublished work on 3D neural networks\n\nDeakin University CRICOS Provider Code: 00113B\n\nWarren McCulloch (L) and Walter Pitts (R)\nSource:\nhttps://donaldclarkplanb.blogspot.com/2021/11/mccullochpitts-neural-nets.html\n\n\fArtificial Intelligence Throughout\nHistory\nFrank Rosenblatt\u2019s Perceptron (1957)\n\u2022 An electronic device based on a single neuron, able to\nlearn to classify 20x20 images.\n\u2022 Marvin Minsky showed in his 1969 book Perceptrons that\nperceptrons were fundamentally limited.\n\nFrank Rosenblatt\nSource: http://csgrad.science.uoit.ca/courses/ist/notebooks/nnhistory.html\n\n\u2022 A few years later, this problem was addressed by the\ndevelopment of multi-layer perceptrons.\n\n10\nDeakin University CRICOS Provider Code: 00113B\n\nMarvin Minsky\nSource: https://en.wikipedia.org/wiki/Marvin_Minsky\n\n\fArtificial Intelligence Throughout\nHistory\nAI Winter (1969-2006)\n\u2022 Minsky & Papert\u2019s findings in Perceptrons (1969) significantly impacted the field\n\u2022 Funding cuts, fewer and fewer researchers working on AI\n\n\u2022 The community moved to more grounded approaches like support vector\nmachines (SVM)\n\u2022 Meanwhile, a few researchers still persevered\n\u2022 And PC gaming led to more and more powerful\nGPUs\n11\nDeakin University CRICOS Provider Code: 00113B\n\nGeoff Hinton (Deep learning), Richard Sutton (Reinforcement\nlearning) and Jurgen Schmidhuber (LSTMs)\n\n\fArtificial Intelligence Throughout\nHistory\nAI Summer (2006-now)\n\u2022 Deep Belief Nets (2006) by Hinton\n\u2022 AlexNet (2012) showed significant performance improvements in the ImageNet\nchallenge (database of over 20000 object categories for visual object recognition)\n\n\u2022 Deepmind: super-human Atari playing capabilities (2015)\n\u2022 Transformers, GPT (2021), Sora (2024), DeepSeek (2025)\n\n\u2022 A very hot AI summer!\n12\nDeakin University CRICOS Provider Code: 00113B\n\n\fRL: Relevance\n\nKeywords based on ICLR2022 data\n\nGoogle trends: Reinforcement Learning\n\n\fRL: popularity\n\nGoogle trends: Reinforcement Learning\n\n\fSIT796 Reinforcement Learning\n\nMachine and Reinforcement Learning\n\nPresented by:\nDr. Thommen George Karimpanal\nSchool of Information Technology\nDeakin University CRICOS Provider Code: 00113B\n\n\fMachine and Reinforcement Learning\nArtificial Intelligence\nSystems designed to behave in such a way that they appear intelligent.\n\u2022 Reasoning (logical Inference)\n\u2022 Learning (Machine Learning)\n\u2022 \u2026\n\nSupervised Learning\nAims to generalize training that is tagged with the\ncorrect answer to solve previously unseen data.\n\nSystems that learn and adapt over time based on\ninstances of external information\n\nSemi-Supervised Learning\nAims are similar to Supervised Learning but uses a combination of tagged and untagged data. Used in areas like\nanomaly detection where certain tags may be unavailable.\n\n16\nDeakin University CRICOS Provider Code: 00113B\n\nReinforcement Learning (RL)\n\nMachine Learning\n\nAgents\u2019 aim to learn optimal behaviour in sequential\ndecision-making tasks through trial-and-error rather\nthan through tagged examples.\n\nUnsupervised Learning\nAims to find pattern using untagged data and can be\nused to identify trends or unknown groupings.\n\n\fMachine and Reinforcement Learning\n\n17\nDeakin University CRICOS Provider Code: 00113B\nhttp://www0.cs.ucl.ac.uk/staff/d.silver/web /Teaching_files/intro_RL.pdf\n\n\fExamples of Reinforcement Learning (1) \u2013 TD Gammon\n\nTD Gammon (1992).\n\u2022 Finished in the top 10 players of the world\n\u2022 While not called it at the time \u2013 TD Gammon was an early success\nof Deep RL.\n\u2013\n\u2013\n\nUsed a multilayer neural network\nUsed Temporal difference learning\n\u2022 Played unique strategies that expert ended up adopting\n\u2013 Eg with a roll of 2-1, 4-1 or 5-1 expert typically used a technique\ncalled \u201cslotting\u201d move from point 6 to point 5.\n\u2013\n\n\u2013\n\nExpert, Bill Robertie did a rollout analysis of TD-Gammon\u2019s \u201csplit\u201d\napproach of moving from point 23 to 24, and found it was more\neffective.\nThis is now the standard opening move\n\n18\nDeakin University CRICOS Provider Code: 00113B\n\nhttps://bkgm.com/articles/tesauro/tdl.html\n\n\fExamples of Reinforcement Learning (2) \u2013 DeepMind Games\n\nAgent57\n\u2022\n\nCould out-perform human benchmarks on all Atari57 games.\n\nAlpha Go (2015).\n\u2022\n\nUsed Monte Carlo Tree search (Coulom 2006) and learnt from both self and\nhuman games.\n\n\u2022\n\nDefeated Lee Sedol 4 games to 1 (2016). First program to beat a 9-dan Go\nchampion on a 19x19 board without a handicap.\n\n\u2022\n\nTelevised internationally, and a documentary movie released.\n\nSutton and Barto (2018)\n\n19\nDeakin University CRICOS Provider Code: 00113B\n\nhttps://images.techhive.com/images/article/2016/03/go-game-screen-sho t-2016-03-08-at-8.17.43-pm-pst-100649230-large.jpg\nWinands M.H.M. (2017) Monte-Carlo Tree Search in Board Games. In: Nakatsu R., Rauterberg M., Ciancarini P. (eds) Handbook of Digital Games and\nEntertainmen t Technologies . Springer, Singapore. https://doi.org/10.1007/ 978-981-4560-50-4_27\nhttps://lh3.googleus ercontent.com/ If132Z_OUQW9jZmdgbalW WJK6cRTwGs5-tAbB27nO_MsB_-sqN YN XOSXrZ8frMu_EuVWOj-6uji7nniRgOLUQ4uaz hVnAvRs Fc3y2A=w1440\n\n\fExamples of Reinforcement Learning (2) \u2013 DeepMind Games\n\n20\nDeakin University CRICOS Provider Code: 00113B\n\nhttps://www.youtube.com/watch?v=WXuK6gekU1Y\n\n\fExamples of Reinforcement Learning\n\nAlphaZero (2018).\n\u2022 Can now play different games eg chess and shogi\nMuZero (2019).\n\u2022 Can learn any game without being told anything about the rules\n\u2013 Learnt Go better than Alpha Zero. Better on Atari\n\nChatGPT (2023)\nUses reinforcement learning to\ndecide the more human-like\nresponse (RLHF)\n\n21\nDeakin University CRICOS Provider Code: 00113B\n\n\fExamples of Reinforcement Learning (4)\n\nMany other application areas\n\u2013 Robotic control, autonomous cars, drones, autonomous underwater vehicles, \u2026\n\u2013 Energy plant , manufacturing, warehouse operations, logistics, \u2026\n\u2013 Trading and finance, healthcare, recommendation, marketing,\u2026\n\u2013 Decision support, natural language processing, video captioning, \u2026\n\nhttp://web.eecs .utk.edu/ ~itamar/Papers/IET_ITS_2010.pdf\n\n22\nDeakin University CRICOS Provider Code: 00113B\n\n\fSIT796 Reinforcement Learning\n\nReinforcement Learning Overview\n\nPresented by:\nDr. Thommen George Karimpanal\nSchool of Information Technology\nDeakin University CRICOS Provider Code: 00113B\n\n\fAn Example:\n\nThe agent (robot) knows nothing about the\nenvironment\nPossible actions: left, right, up, down\n\nWhat should it do?\nTry out different actions and see what happens\nTrial and error!\n\n24\nDeakin University CRICOS Provider Code: 00113B\n\n\fMotivation \u2013 Learning from Experience\n\nWe learn a number of skills by trial and error\nBut how and what do we actually learn?\n\n25\nDeakin University CRICOS Provider Code: 00113B\n\nhttps://directadvicefordads.com.au/new-dads/teaching-baby-to-walk/\nhttps://www.kidsafensw.org/safety/road-safety/bikes-and-wheeled-toys/\nhttps://www.euroschoolindia.com/blogs/how-to-play-chess/\n\n\fMotivation \u2013 Learning from Experience\nTwo key challenges\n\u2022 How to act in a way that is beneficial in the long term and not just\nshort term?\n\n\u2022 Temporal credit assignment: which of the previous actions\nwere responsible for an agent\u2019s good/bad performance?\nHard problem!\n26\nDeakin University CRICOS Provider Code: 00113B\n\n\fMotivation \u2013 Learning from Experience\nReinforcement Learning Involves:\n\u2022\n\nOptimization: Find an optimal way of making\ndecisions\n\n\u2022\n\nGeneralization: How well do these decisions\napply to similar situations\n\n\u2022\n\nExploration: Use past experience to execute\noptimal actions, but also ensure that the agent is\nexposed to new experiences\n\n\u2022\n\nDelayed rewards: Consequences of current\ndecisions may be experienced only in the future\n\n27\nDeakin University CRICOS Provider Code: 00113B\n\n\fSIT796 Reinforcement Learning\n\nThe Problem\n\nPresented by:\nDr. Thommen George Karimpanal\nSchool of Information Technology\nDeakin University CRICOS Provider Code: 00113B\n\n\fThe Problem\nFormally, the RL problem is formulated as a Markov Decision Process (MDP)\n\u2022 An MDP is a tuple \ud835\udc40={\ud835\udcae, \ud835\udc9c, \ud835\udcaf, \ud835\udefe, \u211b}\n\u2013\n\n\ud835\udcae - The set of possible states\n\n\u2013\n\n\ud835\udc9c - The set of actions the agent can take\n\n\u2013\n\n\ud835\udcaf - transition probabilities\n\n\u2013\n\n\ud835\udefe - The discount rate or the discount factor.\n\n\u2013\n\n\u211b - A reward distribution function conditioned.\n\nR=0\n0.05\nR=-1\n\n29\nDeakin University CRICOS Provider Code: 00113B\n\nR=0\n\n1.0\n\n0.1\n\nR=0\nReward function\n\n0.8\n\n0.05\n\nDeterministic transition\n\nProbabilistic transition\n\n\fThe Environment\nAn agent is the learner and decision-maker \u2013 the environment is everything outside the agent that the agent\ndoes not directly control\n\u2022\n\u2022\n\u2022\n\u2022\n\nThe boundary between agent and the environment may not be the physical boundary.\nFor example, a robot\u2019s sensors, motors and actuators, while physically part of the robot, are often treated as part of the environment.\nThis is the same as separating the human mind (agent) from the eyes, ears, skin, muscles and bones (environment)\nA simple rule is that anything that cannot be changed arbitrarily by the agent is part of its environment\n\n30\nDeakin University CRICOS Provider Code: 00113B\nhttps://data61.csiro.au/~/ media/D61/Images/weaver-outdoor-incline.jpg?mw=1600&hash=8E899FC8780FE865A065C9C229D0438106914166\n\n\fState\nThe state is the agent\u2018s internal representation\n\u2022\n\nMay only be a small portion of the environment.\n\nFor example:\n\u2022\n\u2022\n\nAgents generally operate in discrete time and hence perceive the environment as snap shots \u2013 like a film.\nLocation of chess pieces / number of pieces it attacks / places the piece can move.\n\nThe choice of state representation can significantly affect the agent\u2019s ability to learn a solution.\n\nMarkov Property: All of the agent\u2019s history is\nrepresented by the state\n\n31\nDeakin University CRICOS Provider Code: 00113B\n\n\fActions\nThese are things the agent can do in the environment\nCan be discrete or continuous\n\n32\nDeakin University CRICOS Provider Code: 00113B\n\n\fReward Function\nThe Reward Function defines the Goal of the Problem being learnt.\nThe reward function determines how much and when/where reward should be given.\n\u2022\n\u2022\n\nNote: the reward function is external to the agent.\nTherefore, the agent cannot alter it\n\nImproperly designed reward functions can lead to undesirable behaviours.\nEg: Rapple=1 (terminal), Rfire=-1 (terminal), Rdefault=0.1\n\nAgent can collect an accumulate infinite rewards by avoiding apple/fire\nstates!\n33\nDeakin University CRICOS Provider Code: 00113B\nhttps://atlas-conten t-cdn.pixelsquid.com/ stock-images/small-wooden-treas ure-chest-RBewxr1-600.jpg\n\n\fValue Function\nThe Value Function is an estimate of the total reward that an agent can expect to\naccumulate in the future in a given state.\n\nMany ways to store/record a value function:\n\n\u2022 State-value function V(s): indicates how valuable it is to be in state s\n\u2022 Action-value function (or Q-value) Q(s,a): indicates how valuable it is to be in\nstate s and take action a\nRL aims to estimate the value/action-value function.\n34\nDeakin University CRICOS Provider Code: 00113B\n\nhttps://en crypted-tbn0.gstatic.com/images?q=tbn:ANd9GcROCpO4FHAnt5qvpb_VOfRhOnUHyTZYQTCM5A&usqp=CAU\nhttps://atlas-conten t-cdn.pixelsquid.com/ stock-images/small-wooden-treas ure-chest-RBewxr1-600.jpg\n\n\fDiscount Factor\n$100 now or $100 after 1 year?\n\nOf course, now!\n\nWe value things in the future less\nR_other=0\n\nR_treasure=1\n\n(s)\n\nNo discounting:\n\nV(s)=0+0+0+1=1\n\nWith discounting (discount factor=0.9):\n\nV(s)=0+0.9*0+(0.9)2*0+(0.9)3*1= 0.729\n\n35\nDeakin University CRICOS Provider Code: 00113B\nhttps://atlas-conten t-cdn.pixelsquid.com/ stock-images/small-wooden-treas ure-chest-RBewxr1-600.jpg\n\n\fAgent\u2019s Policy\nThe value function provides a means to map an agent\u2019s states to the actions\n\u2022\n\u2022\n\nThis mapping is called the agent\u2019s policy, \ud835\udf0b\ud835\udc61 .\nWhere \ud835\udf0b\ud835\udc61 \ud835\udc60, \ud835\udc4e is the probability of selecting action \ud835\udc4e at state \ud835\udc60 at time \ud835\udc61.\n\nYou can think of the policy as being the way the agent has chosen to behave for a given state\n\u2022\n\u2022\n\nIt is this mapping (policy) that RL methods are attempting to learn.\nOnce learnt these probabilities should be the same as the optimal\nprobability for each state, denoted \ud835\udf0b\ud835\udc61\u2217 \ud835\udc60, \ud835\udc4e\n\nRL methods update their policy \u2013 attempting to maximise the\ntotal amount of reward over the long run.\nAfter the value function is learnt, the best (optimal) action\nis simply the one that corresponds to the highest value.\n\n36\n\nBut should the agent simply choose the action with the\nhighest value all the time?\n\nDeakin University CRICOS Provider Code: 00113B\nhttps ://www.goog le. com/m aps/dir/Deak in+University, +W a urn+Ponds+Cam pus, +75+Pig dons +Rd,+W aurn+Ponds+VI C+3216/Deak in+University+ M elbourne+Burwood+Cam pus ,+221+Burwood+Hwy, +Burwood+VIC+3125/@ 38. 0099452, 144.5760167,11. 17z/da ta = !4m14!4m13!1m5!1m 1!1s0x 6a d413298609d90f:0xc0f7d39b2e be86b2!2m 2!1d144.298789!2d -38. 19691!1m 5!1m1!1s 0x 6a d640592c2ddc eb: 0x 805bd52f251bd12!2m2!1d145.1149861!2d -37. 8474187!3e0\n\n\fSIT796 Reinforcement Learning\n\nAction Selection\n\nPresented by:\nDr. Thommen George Karimpanal\nSchool of Information Technology\nDeakin University CRICOS Provider Code: 00113B\n\n\fAction Selection (Exploration vs Exploitation)\nA fundamental issue in Reinforcement Learning is the trade-off between:\n\u2022 Exploration: Search to see if there are other, potentially better, ways to do\nyour task\n\u2022 Exploitation: Keep doing what you have already learnt is best\n\u2013 Select the action with the highest value function\nWe will discuss a number of ways to address this dilemma:\nMulti-armed Bandits\ngreedy, \u03b5-greedy, Upper Confidence Bounds, and Soft-max\nOptimistic Initialisation\n\n38\nDeakin University CRICOS Provider Code: 00113B\nhttps://drek4537l1klr.cloudfront.net/morales/ v-4/Figures/Images_01-04_118.jpg\n\n\fLearning (Updating the Value Function)\nLearning is the process of updating/changing/improving our understanding of the what is the best policy\n\u2022\n\nTells the system the correct answer regardless of the answer actually given by the system.\n\nOne approach is to use the Value Function.\n\u2022 Recall: The Value Function is an estimate of our future expected reward.\n\u2022 So if after executing an action we find we received more/less reward that we expected then we should update our value functio n\n\nTherefore, if our value for a state, \ud835\udc60, at time, \ud835\udc61, was \ud835\udc49 \ud835\udc60\ud835\udc61 , then we can update it using the Bellman equation.\n\ud835\udc49 \ud835\udc60\ud835\udc61 \u2190 \ud835\udc49 \ud835\udc60\ud835\udc61 + \ud835\udefc \ud835\udc49 \ud835\udc60\ud835\udc61+1 \u2212 \ud835\udc49 \ud835\udc60\ud835\udc61 ,\n\nWhere \ud835\udefc is set to a value between 0 < \ud835\udefc \u2264 1. This is called the step-size parameter (or learning rate)\n\ud835\udefc is sometimes set high initially and reduced overtime.\n\n39\nDeakin University CRICOS Provider Code: 00113B\nhttps://drek4537l1klr.cloudfront.net/morales/ v-4/Figures/Images_01-04_118.jpg\n\n\fTasks\nA task in RL is an MDP {S,A,R,T}\n\u2022\n\nRepresents one instance (epoch) of the RL problem\n\nExamples\n\u2022\n\u2022\n\u2022\n\u2022\n\nGame of Chess\nStand up for as long as possible\nMinimise customer wait times\n\nExtinguish a fire\n\nTwo primary types of tasks:\nEpisodic tasks \u2013 is one that has a defined terminal condition when the task\nis completed. Eg: Chess, reach destination, solve problem\nContinuing Tasks \u2013 tasks with no terminal condition. Eg: moving in a circular\npath, learning mathematics\n40\nDeakin University CRICOS Provider Code: 00113B\n\nhttp://anji.sourceforge.net/polebalance.htm\nhttps://media.wired.co m/pho tos /5f592bfb643fbe1f6e6807ec/16:9/w_2400,h_1350,c_limit/busines s_chess _1200074974.jp g\n\n\fPrediction vs Control\nReinforcement Learning is used to solve two classes of problem\nThe Prediction Problem\n\u2022\n\nAims to learn what value a particular state has \ud835\udc49\ud835\udc61 \ud835\udc60 or the value of an action\nwhen taken from a state \ud835\udc49\ud835\udc61 \ud835\udc60, \ud835\udc4e usually given some example policy.\n\n\u2022\n\nThe values learnt are not used to decide on what the agent will do \u2013 just to\npredict what the outcome will be for different policies\n\n\u2022\n\nFor instance, predicting global warming given different potential policies.\n\nThe Control Problem\n\u2022\n\u2022\n\u2022\n\u2022\n\nAims to learn a control policy that identifies the best action to take given a\nparticular state \ud835\udf0b\ud835\udc61 \ud835\udc60, \ud835\udc4e .\nUsed in decision-making tasks\nSuch methods need to continually update and improve the policy based on\npast experiences.\nFor instance, controlling a robot for picking up rubbish.\n\n41\nDeakin University CRICOS Provider Code: 00113B\n\nhttps://earthobservatory.nasa.gov/ContentFeature/GlobalWarming/images/ipcc_scenarios.png\n\nhttps://robotschampion.com/wp-content/uploads/2018/10/micropsi-industries_technology_02_micropsi_technologies_welcome_ai.jpg\n\n\fA Simple Maze Example (1)\nEpisodic task. Using a policy provided the aim is to learn a prediction of the expected value for each state.\nReward: -1 per time-step\nActions: up, down, left, right\nState: agent\u2019s location\nOptimal Policy \ud835\udf0b \u2217(\ud835\udc60, \ud835\udc4e) represented for each state \ud835\udc60 with a red arrow.\n\n42\nDeakin University CRICOS Provider Code: 00113B\n\n\fA Simple Maze Example (2)\nReward Function: defines the amount of immediate reward.\nValue Function: Stores a number representing the value (expected sum of rewards under a policy) for each\nstate, \ud835\udc63\ud835\udf0b \ud835\udc60\n\n43\nDeakin University CRICOS Provider Code: 00113B\n\n\fReadings\nThis was a quick overview of Reinforcement Learning.\n\u2022 Intention was to introduce the main terminology and the RL\n\u2022\n\u2022\n\nlearning process.\nFollowing topics will delve deeper into the topics discussed here.\nEnsure you understand what was discussed here before doing the\nfollowing topics\n\nFor more detailed information see Sutton and Barto (2018)\nReinforcement Learning: An Introduction\n\u2022 Chapter 1: Introduction\n\u2022 http://incompleteideas.net/book/RLbook2020.pdf\n\n44\nDeakin University CRICOS Provider Code: 00113B\n\n\f",
    "SIT796-2.1P.txt": "SIT796 Reinforcement Learning\nPass Task 2.1: AI powered cleaning robot and value computation\nOverview\n1. You have learnt about the differences between reinforcement learning, supervised\nlearning and unsupervised learning. In this task, consider a futuristic AI-powered\ncleaning robot for household cleaning chores. Discuss features you would incorporate\ninto it. Discuss each feature and describe whether it would use supervised,\nunsupervised or reinforcement learning to achieve it. Make sure you include at least\n1 feature for each learning type (i.e., at least 1 for supervised, 1 for unsupervised and\n1 for reinforcement learning).\n2. For each of these figures, the red arrows indicate the policy \ud835\udf0b. The optimal policy\n(which may or may not be indicated is denoted by \ud835\udf0b \u2217 . Assuming a discount factor of\n\ud835\udefe = 0.9, find the discounted and undiscounted values of state \u2018s\u2019 under the indicated\npolicies. The reward for the treasure state is \ud835\udc5f\" = 1, and for every other state, it is\n\ud835\udc5f#$%&' = \u22120.1.\na) In Fig 1, what are the discounted and undiscounted values \ud835\udc49( (\ud835\udc60) and \ud835\udc49(\u2217 (\ud835\udc60)?\ns\n\nFig 1.\nb) In Fig 2, what are the discounted and undiscounted values \ud835\udc49( (\ud835\udc60) and \ud835\udc49(\u2217 (\ud835\udc60)?\n\ns\n\nFig 2.\n\nSubmission Details\nFor 1., feel free to be creative with regards to your robot\u2019s features. Aim to keep your\nsubmitted essay to at the most 500 words in length. For 2., along with the final values, please\nalso show how you worked out your values. You may refer to slide 35 of week 1 lecture notes\nfor some clues.\n\n\f"
}