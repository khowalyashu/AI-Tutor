Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Deep Reinforcement Learning
Deakin University CRICOS Provider Code: 00113B
Recap: Function approximation
2
State
Action
s1
s2
s3
s4
a1
a2
a3
Q(s1,a1)
Q(s2,a1)
Q(s3,a1)
Q(s4,a1)
Q(s1,a2)
Q(s2,a2)
Q(s3,a2)
Q(s4,a2)
Q(s1,a3)
Q(s2,a3)
Q(s3,a3)
Q(s4,a3)
ğœ‹âˆ—
Function approximators 
(Linear, RBF, NN)
Neural Networks as a function approximator
â€¢
Rumelhart 1986 â€“ great early success
â€¢
Interest subsides in the late 90â€™s as other models are introduced â€“ SVMs, 
Graphical models, etc. 
â€¢
Convolutional Neural Nets â€“ LeCun ,1989 ,for Image recognition, speech, etc. 
â€¢
Deep Belief nets (Hinton) and Stacked auto-encoders (Bengio) in 2006
â€¢
Unsupervised pre-training followed by supervised training
â€¢
Good feature extractors.
â€¢
2012 Initial successes with supervised approaches which overcome vanishing gradient
3  |
Antonio Robles-Kely
What is a Deep Network?
4  |
Krizhevsky, Sutskever, Hinton â€” NIPS 2012
Lion
Antonio Robles-Kely
Intuitionâ€¦
â€¢
First layer learns 1st order features (e.g. edgesâ€¦)
â€¢
Higher order layers learn combinations of features (combinations of edges, 
etc.)
â€¢
Some models learn in an unsupervised mode and discover general features of 
the input space â€“ serving multiple tasks related to the unsupervised instances 
(image recognition, etc.)
â€¢ Final layer of transformed features are fed into supervised layer(s)
â€¢ And entire network is often subsequently tuned using supervised training of 
the entire net, using the initial weightings learned in the unsupervised phase
5  |
â€¢
Neural networks have been shown to be a universal function 
approximator (it can approximate any function given enough data 
and model complexity) 
6  |
Antonio Robles-Kely
Properties
â€¢
Convolutional nets â€“ for image inputs, MLP: for other type of 
inputs, Time series: Recurrent NNs etc.,
â€¢
It is after all, a supervised learning technique. So data must be 
i.i.d. (Independent and identically distributed)
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Deep Nets and Reinforcement Learning
What is Deep Reinforcement Learning?
â€¢
Deep reinforcement learning is standard reinforcement learning where a deep neural 
network is used to approximate either a policy or a value function
â€¢
Deep neural networks require lots of real/simulated interaction with the environment to 
learn
â€¢
Lots of trials/interactions is possible in simulated environments
â€¢
We can easily parallelise the trials/interaction in simulated environments
â€¢
We cannot do this with robotics (no simulations) because action execution takes time, 
accidents/failures are expensive and there are safety concerns
Deep Q-Networks (DQN)
â€¢
It is common to use a function 
approximator Q(s, a; Î¸) to 
approximate the action-value 
function in Q-learning
â€¢
Deep Q-Networks is Q-learning with a 
deep neural network function 
approximator called the Q-network
â€¢
Discrete and finite set of actions A
â€¢
Example: Breakout has 3 actions â€“ 
move left, move right, no movement
â€¢
Uses epsilon-greedy policy to select 
actions
Q-Networks
â€¢
Core idea: We want the neural network to learn a non-linear hierarchy of features 
or feature representation that gives accurate Q-value estimates
â€¢
The neural network has a separate output unit for each possible action, which gives the 
Q-value estimate for that action given the input state (Can also be coded such that each 
state-action pair produces one Q value as output)
â€¢
The neural network is trained using mini-batch stochastic gradient updates and 
experience replay
- Go though batches in the dataset
- These batches make up for an epoch
- Go through several epochs until convergence
Experience Replay
â€¢ Experience is a sequence of states, actions, rewards and next states et = (st, at, 
rt, st+1)
â€¢ Store the agentâ€™s experiences at each time step et = (st, at, rt, st+1) in a dataset 
D = e1, ..., en pooled over many episodes into a replay memory
â€¢ In practice, only store the last N experience tuples in the replay memory and 
sample when performing updates
â€¢ These experiences occur in sequence. So need to randomise them to make them i.i.d.
â€¢ It may require a lot of experience to obtain enough samples.
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Deep Q-Learning
Q-Network Training
â€¢
Sample random mini-batch of experience tuples uniformly at random from D 
(replay buffer)
â€¢
Similar to Q-learning update rule but: 
- Use mini-batch stochastic gradient updates
- The gradient of the loss function for a given iteration with respect to the 
parameter Î¸i is the difference between the target value and the actual value is 
multiplied by the gradient of the Q function approximator Q(s, a; Î¸) with respect 
to that specific parameter
â€¢
Use the gradient of the loss function to update the Q function approximator
Loss Function Gradient Derivation
DQN Algorithm
Regular Q learning update:
DQN in Practice: Trick 1 â€“ Experience 
Replay
â€¢
It was previously thought that the combination of simple online reinforcement 
learning algorithms with deep neural networks was fundamentally unstable
â€¢
The sequence of observed data (states) encountered by an online reinforcement 
learning agent is non-stationary and online updates are strongly correlated
â€¢
The technique of DQN is stable because it stores  the agentâ€™s data in experience 
replay memory so that it can be randomly sampled from different time-steps
â€¢
Aggregating over memory reduces non-stationarity and decorrelates updates but 
limits methods to off-policy reinforcement learning algorithms
â€¢
Experience replay updates use more memory and computation per real 
interaction than online updates, and require off-policy learning algorithms that 
can update from data generated by an older policy 
DQN in Practice: Trick 2
Source: https://huggingface.co/learn/deep-rl-course/unit3/deep-q-algorithm
Use two networks: a 
policy network and 
target network
Freeze the target 
network and 
update it only 
after C steps
Tends to stabilize 
learning
DQN in Practice: Trick 3
Clip rewards to some fixed range [-1,1]
Not so important, but it helps
Using these 3 â€œtricksâ€, DQN training became stable
No convergence guarantees! 
Converges to a local optimum that 
is not far from the global optimum
DQN Example: Playing Atari Games
Presentation title  |  Presenter name
19  |
Mnih et al. (2015). Human-level control through deep reinforcement learning
â€¢
The input is the 8x8 
image region about the 
current position of the 
snake.
â€¢
Q-network with 3 
convolutional layers of 
size
â€¢
32x8x8;stride 4 
â€¢
64x4x4;stride 4
â€¢
64x3x3;stride 2  
â€¢
The final two layers are 
fully connected layers 
with 512
DQN Example: Playing Atari Games
20  |
>= Humans
Poor sample efficiency
Each game learned from 
scratch
DQN Example: Playing Atari Games
21  |
DQN Example: Playing Atari Games
22  |
Better than human performance
May need excessive data ~ 109 samples (if 1s per sample, then >31 years!)
Still needs actions to be discrete â€“ not feasible in many cases
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Dealing with continuous actions
RL algorithm types
Three approaches to find RL policy:
Agent
Environment
r (reward) 
sâ€™ 
(next state) 
1. Value-based methods 
(everything covered so far, 
incl. DQN)
2. Directly obtain policy 
(policy gradient methods)
3. Actor-critic methods 
(combination of 1. and 2.)
Policy Gradient Methods
â€¢
Several kinds:
â€¢
Finite Difference Policy Gradient
â€¢
Monte Carlo Policy Gradient
â€¢
Actor-Critic Policy Gradient
â€¢
Directly parameterize and learn the policy
 
ğœ‹ğ‘ğ‘ = ğ‘“! ğœ™(ğ‘ , ğ‘)
â€¢
Can have several forms such as:
                                                        ğœ‹ğ‘ğ‘ âˆexp ğœƒ!ğœ™(ğ‘ , ğ‘)
Feature vector of state-action pair
Why not directly define ğœ‹ğ‘ğ‘ âˆexp( ğœ†ğ‘£ğ‘ , ğ‘)?
Temperature of soft-max
No need to be related to the value function!
What are Policy Gradient Methods?
â€¢
Before: Learn the values of actions and then select actions based on their estimated 
action-values. The policy was generated directly from the value function
â€¢
We want to learn a parameterised policy that can select actions without consulting a 
value function. The parameters of the policy are called policy weights
â€¢
A value function may be used to learn the policy weights but this is not required for 
action selection
â€¢
Policy gradient methods are methods for learning the policy weights using the gradient 
of some performance measure with respect to the policy weights
â€¢
Policy gradient methods seek to maximise performance and so the policy weights are 
updated using gradient ascent
Policy-based Reinforcement Learning
â€¢
Search directly for the optimal policy Ï€*
â€¢
Can use any parametric supervised machine learning model to learn policies 
Ï€(a |s; Î¸) where Î¸ represents the learned parameters 
â€¢
Recall that the optimal policy is the policy that achieves maximum future 
return
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Policy Approximation
29
1. Start with a random value 
     of w (e.g. w = 12)
2. Compute the gradient 
    (derivative) of L(w) at point
     w = 12. (e.g. dL/dw = 6)
3. Recompute w as:
ğ‘¤ =  ğ‘¤ â€“ ğœ†(ğ‘‘ğ¿(ğ‘¤) / ğ‘‘ğ‘¤)
ğ¿ğ‘¤
ğ‘¤
w=12
Gradient Descent
â€¢
Optimizer for functions.
â€¢
Guaranteed to find optimum for convex functions.
â€¢
Non-convex = find local optimum.
â€¢
Works for multi-variate functions.
â€¢
Need to compute matrix of partial derivatives (â€œJacobianâ€)
30
Policy Gradient: General Idea
Directly learn policy from  objective function:
ğ½ğœƒ= ğ”¼![ğ‘Ÿ(Ï„)]
Ï„: trajectory of (s,a,r) pairs
Directly maximize ğ½(ğœƒ) : obtain the update equation:
ğœƒ"#$ = ğœƒ" + ğ›¼âˆ‡ğ½(ğœƒ")
Policy gradient theorem: It can be shown that:
âˆ‡ğ½(ğœƒ!) âˆ'
"
ğœ‡(ğ‘ ) '
#
ğ‘$(ğ‘ , ğ‘)âˆ‡ğœ‹(ğ‘|ğ‘ , ğœƒ!)
Refer to textbook for derivation. 
It forms the basis for the policy 
gradient family of methods
Policy Approximation
â€¢ Basic assumption: policy is differentiable w.r.t. Î¸ . Eg:
soft-max in action 
preferences
â€¢ Action preferences could also be linear:
â€¢ For some problems, it is simpler to learn the policy directly rather than learning the 
value functions, and extracting the policy from it later.
Notation
â€¢ The policy is Ï€(a | s, Î¸), which represents the probability that action a is taken in state 
s with policy weight vector Î¸
â€¢ If using learned value functions, the value functionâ€™s weight vector is w
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
REINFORCE
REINFORCE
Using the policy gradient theorem, the update rule can be modified to be:
ğœƒ"#$ = ğœƒ" + ğ›¼ğ›¾"ğº"âˆ‡ğ‘™ğ‘›ğœ‹(ğ‘"|ğ‘ ", ğœƒ")
where                  ğº" = ?
%&"#$
'
ğ›¾%("($ ğ‘…%
Iteratively updating ğœƒ with the above update rule will lead to the optimal policy ğœ‹âˆ—
REINFORCE Properties and Algorithm
â€¢
On-policy method based on SGD
â€¢
Uses the complete return from time t, which includes all future rewards until the end of 
the episode
â€¢
REINFORCE is thus a Monte Carlo algorithm and is only well-defined for the episodic case 
with all updates made in retrospect after the episode is completed
Actor-Critic Methods
Actor-critic methods are a fusion of policy gradient and value-based methods
Actor: deals with the policy (policy gradient-based)
ğœƒ" is updated to minimise ğ›¿" with the update rule:
Critic: evaluates the actorâ€™s action (value-based)
ğ›¿" = ğ‘Ÿ"#$ + ğ›¾ğ‘‰ğ‘ "#$ âˆ’ğ‘‰(ğ‘ ")
ğ‘"~ğœ‹%
ğœƒ" â†ğœƒ" + ğ›½ğ›¿"
where ğ›½ is a positive step size hyperparameter
Modern approaches include several variants of the actor-critic algorithm. 
Both actor and critic are updated using ğ›¿! to determine the optimal policy ğœ‹âˆ—  
One-step Actor-Critic Update Rules
â€¢
On-policy method
â€¢
The state-value function update rule is the TD(0) update 
rule
â€¢
The policy function update rule is shown below.
One-step Actor-Critic Algorithm
Modern Algorithms
â€¢
Trust Region Policy Optimisation
â€¢
Proximal Policy Optimisation (Simplification of TRPO)
â€¢
Soft Actor-Critic (SAC)
â€¢
DDPG (Deep deterministic policy gradient)
â€¢
and many more!
Deakin University CRICOS Provider Code: 00113B
This lecture focused on introducing Deep Learning for RL.
â€¢
Future topics will expand on this topic by looking at particular methods in Deep RL. 
â€¢
Ensure you understand what was discussed here before doing the following topics
For more detailed information see:
â€¢
https://www.ics.uci.edu/~dechter/courses/ics-295/fall-
2019/texts/An_Introduction_to_Deep_Reinforcement_Learning.pdf
â€¢
https://rail.eecs.berkeley.edu/deeprlcourse/
â€¢ Other Readings:
â€¢
 Playing Atari with Deep Reinforcement Learning (https://arxiv.org/abs/1312.5602)
Readings
40
