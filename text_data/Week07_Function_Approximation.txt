Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Function Approximation
Deakin University CRICOS Provider Code: 00113B
Tabular approach:
â€¢
Single entry in a table of values
â€¢
States and actions have discrete representations
â€¢
Working set of algorithms
â€¢
Convergence proofs
Approach so far
2
State
Action
s1
s2
s3
s4
a1
a2
a3
Q(s1,a1)
Q(s2,a1)
Q(s3,a1)
Q(s4,a1)
Q(s1,a2)
Q(s2,a2)
Q(s3,a2)
Q(s4,a2)
Q(s1,a3)
Q(s2,a3)
Q(s3,a3)
Q(s4,a3)
ğœ‹âˆ—
Deakin University CRICOS Provider Code: 00113B
Limitations of Tabular Approach
3
State
Action
s1
s2
s3
s4
a1
a2
a3
Q(s1,a1)
Q(s2,a1)
Q(s3,a1)
Q(s4,a1)
Q(s1,a2)
Q(s2,a2)
Q(s3,a2)
Q(s4,a2)
Q(s1,a3)
Q(s2,a3)
Q(s3,a3)
Q(s4,a3)
ğœ‹âˆ—
Function approximators 
(Linear, RBF, NN)
Deakin University CRICOS Provider Code: 00113B
Letâ€™s first consider state value prediction ğ‘£! with function approximation. 
â€¢
Instead of representing each state in a table of values, we represent it in a parameterised 
functional form using a weight vector ğ°âˆˆâ„!
â€¢
Hence, we write the value for the approximate value of state ğ‘  as given the weight vector ğ°: 
%ğ‘£ğ‘ , ğ°â‰ˆğ‘£"(ğ‘ )
â€“
The number of dimensions of the vector is strictly much less the number of raw states ğ‘‘â‰ªğ’®.
â€“
Hence changing the value of one weight changes the value of many weights
â€“
Problem now is to find a ğ°* that best approximates ğ‘£"(ğ‘ ) (or ğ‘„"(ğ‘ , ğ‘))
Function Approximation
4
https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png
https://miro.medium.com/max/3840/1*jojTznh4HOX_8cGw_04ODA.png
Examples of what "ğ‘£ might be
Could be a linear function of features in the state where ğ° is the weight of each feature
Could be non-linear function of features in the state computed over an Artificial Neural Network
Could be function computed by a decision tree where ğ° is all the numbers defining the split points of the 
tree
It worth noting that the new notation can still represent the tabular approaches seen so far. 
For instance, represent the state as a vector of the table entries. The feature representing the location of the agent is set to 1 and all others to 0.
Deakin University CRICOS Provider Code: 00113B
A special case of function approximation commonly used Linear function approximation. 
â€¢
That is our approximation function %ğ‘£0, ğ° is linear function over the weight vector ğ°.
â€¢
Where each state ğ‘  is represented with a vector x ğ‘ = ğ‘¥# ğ‘ , ğ‘¥$ ğ‘ , â‹¯, ğ‘¥! ğ‘ 
%, where ğ±ğ‘ 
= ğ°
â€¢
Now we can represent our state-value function using the inner product of ğ° and x ğ‘ 
%ğ‘£ğ‘ , ğ°= ğ°%x ğ‘ = 6
&'#
!
ğ‘¤&ğ‘¥& ğ‘ 
â€¢
Here x ğ‘  is referred to as the feature vector for the state ğ‘ .
Linear Methods
5
G
x ğ‘ = [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]
ğ°= [0.0,0.1,0.2,0.4,0.1,0.3,0.4,0.6,0.2,0.4,0.6,0.8,0.4,0.6,0.8,1.0]
%ğ‘£ğ‘ , ğ°= ğ°%x ğ‘ = 0.3
Deakin University CRICOS Provider Code: 00113B
Probably the simplest approach for function approximation is State Aggregation. 
â€¢
The aim is simply to group similar states together and treat them as a single state
â€¢
The grouped states are represented with a single value within the weight vector 
State Aggregation
6
https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png
https://miro.medium.com/max/3840/1*jojTznh4HOX_8cGw_04ODA.png
For example
Image we have a grid world with 10,000 grids in two dimensions = 100 million discrete states.
Recall the more states then the more values to learn and less often we get to visit them relative to 
all the states
Now if we know an action may move us between one step up to 100 grids during each time step 
then we may group 50 states of each dimension as a single state. 
This will reduce our states to 200Ã—200 = 40,000 states
Faster learning but each group of 
50 states will each be assigned 
a single common value.
This is 1 state now
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Stochastic Gradient MC
Deakin University CRICOS Provider Code: 00113B
In Gradient Descent our aim is to reduce the error of all our examples. 
â€¢
However, this is not useful in RL because we do not have all the examples when learning online through interaction.
Stochastic Gradient Descent (SGD) allows us to use a single example by adjusting our weight vector ğ° in the direction 
of the estimated error, governed by a small factor ğ›¼. 
â€¢
To do this we must be able to identify the direction of the error, hence our function %ğ‘£ğ‘ , ğ° must be differentiable.
â€“
We find the slope of a function by finding its partial derivative in this case with respect to ğ°
â€¢
Now when calculating an update we move the weight vector towards a 
smaller error by moving a small amount in the direction of the error. 
â€¢
In the linear case this reduces to:
E.g. the gradient for the example on the previous slide is 
Stochastic Gradient MC
8
Input: 
 
the policy Ï€ to be evaluate 
 
A differentiable function "ğ‘£: ğ’®Ã—â„! â†’â„
Algorithm Parameter: 
 
Step size ğ›¼âˆˆ(0,1]
Initialise:  
 
ğ°âˆˆâ„! arbitrarily e.g. ğ°= 0
Loop forever (for each episode):
 
Generate an episode based on Ï€: ğ‘†", ğ´", ğ‘…#, ğ‘†#, ğ´#, ğ‘…$, â€¦ , ğ‘†%&#, ğ´%&#, ğ‘…%
 
Loop for each step of the episode ğ‘¡= 0,1, â€¦ , ğ‘‡âˆ’1:
 
 
ğ°= ğ°+ ğ›¼ğº' âˆ’"ğ‘£ğ‘†', ğ°âˆ‡"ğ‘£ğ‘†', ğ° 
âˆ‡ğ‘“ğ°=
ğœ•ğ‘“ğ°
ğœ•ğ‘¤&
, ğœ•ğ‘“ğ°
ğœ•ğ‘¤&
, â‹¯, ğœ•ğ‘“ğ°
ğœ•ğ‘¤&
%
ğ°()#  = ğ°( + ğ›¼ğº( âˆ’%ğ‘£ğ‘†(, ğ°(
âˆ‡%ğ‘£ğ‘†(, ğ°(
âˆ‡%ğ‘£ğ‘†(, ğ°( = [0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0]
Stochastic Gradient Descent for MC Prediction
ğ°()#  = ğ°( + ğ›¼ğº( âˆ’%ğ‘£ğ‘†(, ğ°(
x ğ‘ 
Deakin University CRICOS Provider Code: 00113B
When using applying these ideas to bootstrapping methods (e.g. TD) then we are basing updates on an estimate rather 
than the true value. 
Semi-gradient methods need to replace the true target with an estimate of the target
Semi-Gradient ğ‘»ğ‘«(ğŸ)
9
Input: 
 
the policy Ï€ to be evaluate 
 
A differentiable function "ğ‘£: ğ’®Ã—â„! â†’â„
Algorithm Parameter: 
 
Step size ğ›¼âˆˆ(0,1]
Initialise:  
 
ğ°âˆˆâ„! arbitrarily e.g. ğ°= 0
Loop forever (for each episode):
 
Initialize ğ‘†
 
Loop for each step of the episode until ğ‘†âˆˆS()*+,-./0) :
 
 
Choose ğ´~ğœ‹(@ |ğ‘†)
 
 
Take action ğ´, observe ğ‘…, ğ‘†2
 
 
ğ°= ğ°+ ğ›¼ğ‘…+ ğ›¾"ğ‘£ğ‘†2, ğ°âˆ’"ğ‘£ğ‘†, ğ°âˆ‡"ğ‘£ğ‘†, ğ°
 
 
ğ‘†â†ğ‘†2
ğº( = ğ‘…()# + ğ›¾%ğ‘£ğ‘†"#$, ğ°
Why semi-gradient?
When we update w, we take into 
account the change in estimate, but 
donâ€™t take into account the change in 
the target (which also depends on w)
So the gradient computed only 
represents part of the true gradient
Stability issues!
Deakin University CRICOS Provider Code: 00113B
Recall we were able to use eligibility traces to define an algorithm that sat between MC and TD(0). 
â€¢
We obviously also wish to have this ability when using function approximation.
â€¢
This will allow us to update the weight vector each iteration, balance computation throughout instead of just at the end, and allow its 
application to continuing problems.
Semi-Gradient ğ‘»ğ‘«(ğ€)
10
Input: 
 
The policy Ï€ to be evaluate 
 
A differentiable function "ğ‘£: ğ’®Ã—â„! â†’â„
Algorithm Parameter: 
 
Step size ğ›¼âˆˆ(0,1]
 
Trace decay rate ğœ†âˆˆ[0,1]
Initialise:  
 
ğ°âˆˆâ„! arbitrarily e.g. ğ°= 0
Loop forever (for each episode):
 
Initialize ğ‘†
 
Reset ğ³= ğŸ
 
Loop for each step of the episode until ğ‘†âˆˆS()*+,-./0) :
 
 
Choose ğ´~ğœ‹(@ |ğ‘†)
 
 
Take action ğ´, observe ğ‘…, ğ‘†2
 
 
ğ³â†ğ›¾ğœ†ğ³+ âˆ‡"ğ‘£ğ‘†, ğ°
 
 
ğ›¿â†ğ‘…+ ğ›¾"ğ‘£ğ‘†2, ğ°âˆ’"ğ‘£ğ‘†, ğ°
 
 
ğ°= ğ°+ ğ›¼ğ›¿ğ³
 
 
ğ‘†â†ğ‘†2
Instead of defining a  trace for each state, we 
define it as a vector ğ³" âˆˆâ„%, ğ‘ . ğ‘¡. ğ³= ğ°
 
ğ³*# = 0
 
ğ³( = ğ›¾ğœ†ğ³(*# + âˆ‡%ğ‘£ğ‘†(, ğ°ğ’•,  
0 â‰¤ğ‘¡â‰¤ğ‘‡
         
        Now we can update the weight vector proportionally 
to the trace vector
 
ğ°()# = ğ°( + ğ›¼ğ›¿(ğ³(
        Where the TD-error is calculated the same as ğ‘‡ğ·(0)
 
 
ğ›¿( = ğ‘…()# + ğ›¾%ğ‘£ğ‘†()#, ğ°ğ’•âˆ’%ğ‘£ğ‘†(, ğ°ğ’•
        Instead of having to iterate through each ğ‘’ğ‘  as we 
did in the tabular case, we simply update the 
trace vector in a single operation
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Function approximation with Control
Deakin University CRICOS Provider Code: 00113B
Now that we have a method of prediction, we can investigate a method of control. 
â€¢
Now we want to approximate the action-value function %ğ‘â‰ˆğ‘".
â€¢
That is, we will represent the parameterized functional form using a weight vector ğ°.
Hence, One-step Sarsa with function approximation is defined with the update rule
Semi-Gradient Control
12
ğ°ğ’•)ğŸ= ğ°( + ğ›¼ğ‘…()# + ğ›¾%ğ‘ğ‘†()#, ğ´()#, ğ°ğ’•âˆ’%ğ‘ğ‘†(, ğ´(, ğ°ğ’•
âˆ‡%ğ‘ğ‘†(, ğ´(, ğ°ğ’•
Control, however, is not just mapping the state to our vector ğ’˜ but also our actions.
 
 
For discrete actions this is perfectly fine â€“ use a matrix where each column represents one of the possible actions.
 
 
Can also use a function approximation, such as action aggregation approach on the actions 
 
 
However, every action increases the dimensionality of our value function
 
  Neither of these are always suitable â€“ Sometimes we want to have precise actions
 
 
E.g. The angle we turn the steering wheel on a car must be very precise to avoid an accident
 
 
This type of continuous action control  is very much still an open question.
 
  For now, we will assume there is a manageable discrete set of actions to select from
Deakin University CRICOS Provider Code: 00113B
Semi-Gradient ğ‘ºğ’‚ğ’“ğ’”ğ’‚(ğŸ)
13
Input: 
 
A differentiable state-action value function "ğ‘: ğ’®Ã—ğ’œÃ—â„! â†’â„
 
A policy ğœ‹ if predicting or ğ‘3 if estimating (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
Algorithm Parameter: 
 
Step size ğ›¼âˆˆ(0,1]
Initialise:  
 
ğ°âˆˆâ„! arbitrarily e.g. ğ°= 0
Loop forever (for each episode):
 
ğ‘†, ğ´â† Initial state and action of episode (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
Loop for each step of the episode until ğ‘†âˆˆS()*+,-./0) :
 
 
Take action ğ´, observe ğ‘…, ğ‘†2 
 
 
If ğ‘†2 âˆˆğ‘†'456789: then:
 
 
 
 ğ°= ğ°+ ğ›¼ğ‘…+ ğ›¾"ğ‘ğ‘†, ğ´, ğ°âˆ‡"ğ‘ğ‘†, ğ´, ğ°, special case for terminal state canâ€™t include future state
 
 
else:
 
 
 
Choose ğ´2 as a function of "ğ‘ğ‘†2,@, ğ° (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
 
 
ğ°= ğ°+ ğ›¼ğ‘…+ ğ›¾"ğ‘ğ‘†2, ğ´2, ğ°âˆ’"ğ‘ğ‘†, ğ´, ğ°âˆ‡"ğ‘ğ‘†, ğ´, ğ°
 
 
 
ğ‘†â†ğ‘†2
 
 
 
ğ´â†ğ´â€²
Sarsa can also be extended to use eligibility traces similar to ğ‘‡ğ·(ğœ†) â€“ Note: This assumes binary features
Deakin University CRICOS Provider Code: 00113B
ğ‘ºğ’‚ğ’“ğ’”ğ’‚ğ€ with Linear Function Approximation
14
Input: 
 
A policy ğœ‹ if predicting or ğ‘! if estimating (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
A feature function â„±ğ‘ , ğ‘ returning the set of active feature indices for ğ‘ , ğ‘
Algorithm Parameter: 
 
Step size ğ›¼âˆˆ(0,1]
 
Trace decay rate ğœ†âˆˆ[0,1]
Initialise:  
 
ğ°âˆˆâ„" arbitrarily e.g. ğ°= 0
Loop for each episode:
 
Initialise ğ‘†
 
Choose ğ´~ğœ‹= ğ‘† initial action of episode (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
Reset ğ³= ğŸ
 
Loop for each step of the episode until S# is a terminal state:
 
 
Take action ğ´, observe ğ‘…, S# 
 
 
ğ›¿â†ğ‘…âˆ’âˆ‘$âˆˆâ„±',) ğ‘¤$
 
 
for all ğ‘–âˆˆâ„±ğ‘†, ğ´:
 
 
 
ğ³â†ğ³+ 1 or ğ³â†1 depending on using accumulating or replacing traces
 
 
If ğ‘†# âˆˆğ‘†*+,-$./0 then:
 
 
 
ğ°â†ğ°+ ğ›¼ğ›¿ğ³
 
 
else:
 
 
 
Choose ğ´#~ğœ‹= S#  action (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
 
 
ğ›¿â†ğ›¿+ ğ›¾âˆ‘$âˆˆâ„±'!,)! ğ‘¤$ 
 
 
 
ğ°â†ğ°+ ğ›¼ğ›¿ğ³
 
 
 
ğ³â†ğ›¾ğœ†ğ³
 
 
 
ğ‘†â†ğ‘†#; 
 
 
 
ğ´â†ğ´#; 
Deakin University CRICOS Provider Code: 00113B
There is a function approximation version of Watkinsâ€™s Q-Learning that has been very popular. 
â€¢
Below is a binary features version of the approach which aligns with the Sarsa implementation.
Semi-Gradient Off-Policy Control
15
Note: for most of this unit we 
are using Sutton and Barto 
(2018) â€“ This algorithm, 
however is based on Sutton and 
Barto (1998) version as they no 
longer include it in the new 
version of the book. The 
reasons discussed on the 
following slides. However, I 
include as it is still in common 
use
Input: 
 
A feature function â„±ğ‘ ,ğ‘ returning the set of active feature indices for ğ‘ ,ğ‘
Algorithm Parameter: 
 
Step size ğ›¼âˆˆ(0,1]
 
Trace decay rate ğœ†âˆˆ[0,1]
Initialise:  
 
ğ°âˆˆâ„" arbitrarily e.g. ğ°= 0
Loop for each episode:
 
Initialise ğ‘†
 
Choose ğ´~ğœ‹= ğ‘† initial action of episode (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
Reset ğ³= ğŸ
 
Loop for each step of the episode until S# is a terminal state:
 
 
Take action ğ´, observe ğ‘…,S# 
 
 
ğ›¿â†ğ‘…âˆ’âˆ‘$âˆˆâ„±',) ğ‘¤$
 
 
for all ğ‘–âˆˆâ„±ğ‘†,ğ´: ğ³â†ğ³+ 1
 
 
for all ğ‘âˆˆğ’œ(ğ‘ ):
 
 
 
ğ‘„/ â† âˆ‘$âˆˆâ„±',/ ğ‘¤$
 
 
ğ›¿â†ğ›¿+ ğ›¾max
/ ğ‘„/
 
ğ°â†ğ°+ ğ›¼ğ›¿ğ³ 
update all weight vectors
 
 
Choose ğ´#~ğœ‹= S#  action (e.g. using ğœ€âˆ’ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦)
 
 
If ğ‘”ğ‘Ÿğ‘’ğ‘’ğ‘‘ğ‘¦_ğ‘ğ‘ğ‘¡ğ‘–ğ‘œğ‘› selected then:
 
 
 
ğ³â†ğ›¾ğœ†ğ³
 
 
else:
 
 
 
 ğ³â†0
 
 
ğ‘†â†ğ‘†#; 
 
 
ğ´â†ğ´#; 
Deakin University CRICOS Provider Code: 00113B
â€œ..the danger of instability and divergence arises whenever we combine all of the following three elements, making up 
what we call the deadly triad.â€
â€¢
Function approximation
â€¢
Bootstrapping
â€¢
Off-Policy training
The Deadly Triad
16
Can we give up one of them? Presence of any 2 appears to be manageable (leads to stable learning)
Function approximation â€“ Can not be replaced in large state spaces without introducing the Curse-of-Dimensionality
Bootstrapping â€“ Is possible but at the cost of significantly more computation and loss of efficiency with increased 
memory costs
Off-Policy training â€“ we can just use on-policy methods instead and often that is good. However, there are many use 
cases where we want to learn multiple policies simultaneously in parallel. 
Watkinsâ€™ Q-Learning with function approximation however has been found to be unstable and does not always converge. 
Deakin University CRICOS Provider Code: 00113B
There are a number of approaches recently that provide stable off-poly methods with function approximation. 
â€¢
For instance, one approach might be to select off-policy behaviours that are close to the target policy can be effective
Gradient-TD methods. 
â€¢
Aim to minimize the Projected Bellman Error instead of reducing the TD-error
â€¢
They achieve this but effectively double the computational complexity
Emphatic-TD methods. 
â€¢
These methods rewrite the state transitions using importance sampling s.t. they are appropriate for learning the target policy
â€“
It does this while using the behaviour policy distributions
These methods however are not explored in these classes. 
â€¢
They can be explored for your major research task
Stable Off-policy Methods
17
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Feature Construction
Deakin University CRICOS Provider Code: 00113B
For function approximation, we need to convert the state-value function to feature vectors. 
This can be done using any of the following:
â€¢
Polynomial features
â€¢
Fourier basis
â€¢
Coarse coding
â€¢
Tile coding
â€¢
Sparse coding
â€¢
Dictionary Learning
Consider the state ğ‘  represented to be represented by a vector x ğ‘ = ğ‘¥$ ğ‘ , ğ‘¥& ğ‘ , â‹¯, ğ‘¥% ğ‘ 
', where ğ±ğ‘ 
= ğ°
We can represent our state-value function using the inner product of ğ° and x ğ‘ 
"ğ‘£ğ‘ , ğ°= ğ°'x ğ‘ = 8
()$
%
ğ‘¤(ğ‘¥( ğ‘ 
Function Construction
19
Deakin University CRICOS Provider Code: 00113B
In a polynomial feature, x ğ‘  is a polynomial basis, that is, for a set of states ğ‘ = {ğ‘ $, ğ‘ &, ğ‘ *, â€¦ , ğ‘ +}, the polynomial 
features can be written as
ğ‘¥( ğ‘ = =
,)$
+
ğ‘ ,
-T,U
where ğ‘(,, is a non-negative integer such that ğ‘(,, = {0, 1, 2, â€¦ , ğ‘›}
Thus, we have
"ğ‘£ğ‘ , ğ°= ğ°'x ğ‘ = 8
()$
%
ğ‘¤( =
,)$
+
ğ‘ ,
-T,U
and ğ‘¥( ğ‘  is a n-order polynomial basis in a k-dimensional space spanned by ğ‘›+ 1 + different features
Polynomial Features
20
Deakin University CRICOS Provider Code: 00113B
There are several kinds of polynomial bases, such as:
â€¢
Lagrange
â€¢
Newton
â€¢
Orthogonal polynomials
â€¢
Chebyshev
The Newton basis functions are:
ğœ‹+ ğ‘¡= =
,)$
+/$
(ğ‘¡âˆ’ğ‘¡,)
Which gives, in the third order case, the following
1
0
0
1
ğ‘¡& âˆ’ğ‘¡$
0
1
ğ‘¡* âˆ’ğ‘¡$
ğ‘¡* âˆ’ğ‘¡$
ğ‘¡* âˆ’ğ‘¡&
ğ‘¤$
ğ‘¤&
ğ‘¤*
=
ğ‘£$
ğ‘£&
ğ‘£*
Polynomial Feature Example
21
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Fourier Basis
Deakin University CRICOS Provider Code: 00113B
â€¢
Polynomials are not the best - unstable and not very physically meaningful.
â€¢
Easier to talk about â€œsignalsâ€ in terms of its â€œfrequenciesâ€ (how fast/often signals change, etc).
â€¢
Any periodic function can be rewritten as a weighted sum of Sines and 
Cosines of different frequencies (Jean Baptiste Joseph Fourier, 1807).
The aim is then to understand the frequency w of our signal.  
So, letâ€™s reparametrize the signal by w instead of x:
For every w from 0 to inf, F(w) holds the amplitude A and phase f of the 
corresponding sine  
Fourier Transform
23
f(x)
F(w)
Fourier 
Transform
)
+f
wx
Asin(
Arbitrary function 
Single Analytic Expression
Spatial Domain (x)
Frequency Domain (u)
Represent the signal as an infinite weighted sum of an infinite number of 
sinusoids
( )
( )
Ã²
Â¥
Â¥
-
-
=
dx
e
x
f
u
F
ux
i p
2
(Frequency Spectrum F(u))
1
sin
cos
-
=
+
=
i
k
i
k
eik
Note:
Inverse Fourier Transform (IFT)
( )
( )
Ã²
Â¥
Â¥
-
=
dx
e
u
F
x
f
ux
i p
2
Fourier Transform
Deakin University CRICOS Provider Code: 00113B
This means that F can encode both 
using the imaginary numbers. 
Consider
The amplitude is given by
and the phase is
Fourier Transform
25
)
(
)
(
)
(
w
w
w
iI
R
F
+
=
2
2
)
(
)
(
w
w
I
R
A
+
Â±
=
)
(
)
(
tan 1
w
w
f
R
I
-
=
Deakin University CRICOS Provider Code: 00113B
In a Fourier basis, the set of states ğ¬= ğ‘ $, ğ‘ &, ğ‘ *, â€¦ , ğ‘ + ğ“ can be expressed using a cosine in the following way
ğ‘¥( ğ‘ = cos(ğœ‹ğ‘ 1ğœ()
where ğœ( = ğ‘$
(, â€¦ , ğ‘+
( 1 such that ğ‘,
( âˆˆ{0, 1, 2, â€¦ , ğ‘›}
Thus, we have:
"ğ‘£ğ‘ , ğ°= ğ°'x ğ‘ = 8
()$
%
ğ‘¤(cos(ğœ‹ğ‘ 1ğœ() 
Fourier Features
26
= 
+
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Coding Methods
Deakin University CRICOS Provider Code: 00113B
28
â€¢
Coarse coding is used for continuous spaces, but 
can also be used for state-spaces that are binary or 
to further encode other features.
â€¢
Each ball or sphere is usually called a receptive field.
â€¢
Features with large receptive fields give broad 
generalization, but might yield very coarse 
approximations
â€¢
The trade-off is often a question to be solved for 
their implementation:
-Too coarse may not be discriminative enough
-Too large and the complexity may increase too much
Coarse Coding
Deakin University CRICOS Provider Code: 00113B
29
The type of receptive field affects the nature of generalisation
Coarse Coding
Deakin University CRICOS Provider Code: 00113B
30
Coarse Coding
Deakin University CRICOS Provider Code: 00113B
31
Tile Coding
â€¢
Makes use of tiles and tilings
â€¢
Tiles are elements of tilings 
â€¢
If only 1 tiling is used â€“ state 
aggregation
â€¢
Many tilings are used to obtain the 
ability to represent the state space as 
finely or coarsely as required 
(Generalisation)
â€¢
Uses binary features
â€¢
Tilings are offset from each other 
uniformly in each dimension
Deakin University CRICOS Provider Code: 00113B
32
Tile Coding
â€¢
Tilings are offset from each other 
uniformly in each dimension
â€¢
Diagonal elements become too 
prominent
â€¢
To fix this, the offset can be done 
unsymmetrically
Deakin University CRICOS Provider Code: 00113B
This lecture focused on exploring how we can use function approximation in RL.
â€¢
Future topics will expand on this topic by looking at Deep RL. 
â€¢
Ensure you understand what was discussed here before doing the following topics
For more detailed information see Sutton and Barto (2018) Reinforcement 
Learning: An Introduction. Several illustrations here were borrowed from the 
book.
â€¢
Chapter 9.5: Feature Construction for Linear Methods
â€¢
http://incompleteideas.net/book/RLbook2020.pdf
Other readings:
â€¢
Isabelle Guyon, Steve Gunn, Masoud Nikravesh, and Lofti Zadeh (Eds.), â€œFeature Extraction: 
Foundations and Applicationsâ€, Springer, 2006.
â€¢
http://www.causality.inf.ethz.ch/ciml/FeatureExtractionManuscript.pdf
â€¢
https://sites.fas.harvard.edu/~cs278/papers/ksvd.pdf
Readings
33
