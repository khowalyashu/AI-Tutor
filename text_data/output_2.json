{
  "SIT796-5.1P.pdf": "SIT796 Reinforcement Learning Pass Task 5.1: Q learning and SARSA Overview During week 5, you have learnt about some TD learning approaches. (a) In the Cliff Walking environment in the Week 5 workshop, SARSA seemed to learn safer policies compared to Q-learning. What is the reason for this? (b) What happens to their performances if you change both algorithms from a fixed epsilon policy to a decaying epsilon policy? Submission Details Keep the answers short and to the point.",
  "SIT796-5.2HD.pdf": "SIT796 Reinforcement Learning High Distinction Task 5.2: TD learning Overview        (a) The above figure shows a room in which a robot starts moving from the shown position. The green lines show the random route taken by the robot while it was exploring the room. The robot manages to explore all areas except the area shaded in gray. We have recorded the trajectory (the states and actions ‚Äì which are assumed to be discrete) taken by the robot along the green lines.  1. Using the collected data, we want to learn a policy to navigate to the goal location g1 (each goal is marked by a pink square region). What algorithm will you use, and how would you go about learning a policy to go to g1? Describe the steps you would take. 2. Now that we have learned how to go to g1, we want to go to g2. Is it really possible to learn another policy (going to g2) with the same data? Again, describe the algorithm you would use and the steps you would take to learn this policy. 3. Now the user wants the robot to also learn a policy to go to g3. Again, is this possible to learn using the same data? Describe how you would learn to go to g3.  (b) In the week 5 workshop, you saw that SARSA learned safer policies compared to Q-learning. Suggest 3 modifications (come up with your own ideas along with the rationale) to the Q-learning algorithm to make it generate safer policies. Implement one of these ideas in the cliff world environment. Compare its performance to SARSA. Is it doing better or worse? Why?  (c) In the cliff world environment, run both Q-learning and SARSA by setting epsilon=1 (always choose random actions). Plot the learning curve (sum of rewards vs episodes). As you just took random actions in the environment, the reward curves just look random, and it probably does not look like the agent is learning. Now, just as you computed and plotted the sum of rewards vs episodes, plot the sum of absolute values of the TD errors in each episode vs episodes. Run both algorithms for the same setting (epsilon=1). Compare these absolute TD error plots for Q-learning and SARSA ‚Äì explain what you observe.   Submission Details Some of these questions have no single correct answer. There may be many possible answers. Include your code in the submissions. Answer as briefly and accurately as possible.",
  "SIT796-7.1P.pdf": "SIT796 Reinforcement Learning Pass Task 7.1: Function Approximation  Overview During Week 7, you have learnt about function approximation.   1. What is the ‚Äòdeadly triad‚Äô in the context of reinforcement learning?   2. What is the principle behind tile coding? What is the effect of increasing the number of tiles and tilings?  Constraints Please keep your answers short and to the point.",
  "SIT796-3.1P.pdf": "SIT796 Reinforcement Learning Pass Task 3.1: Dynamic Programming  Overview During week 3, you were introduced to dynamic programming. Answer the following questions based on your learnings.  Submission Details (a) What are the roles of policy evaluation and policy improvement in policy iteration? (b) What are the main limitations of dynamic programming?  Constraints Keep the explanations short and to the point. Try not to exceed 500-600 words in total.",
  "SIT796-6.2C.pdf": "SIT796 Reinforcement Learning Credit Task 6.2: Eligibility Traces  Overview During Week 6, you have learnt about Eligibility Traces.   1. Your asked your associate to write a function implementing accumulating traces. They show you this: \n Which is the main line that indicates an error in their code? Why?   2. Ask ChatGPT (or another GenAI tool of your choice) to fix this code. Paste the screenshot in your report. Is the code correct? Why/Why not?  3. In the Dyna-Q pseudocode from the book (shown below): \n Can we just replace the highlighted line of code with a SARSA update (for a DYNA-SARSA implementation)? Why/Why not?  Constraints Please keep your answers short and to the point.",
  "SIT796-1.1P.pdf": "SIT796 Reinforcement Learning Pass Task 1.1: Reinforcement Learning Environments  Overview During week 1, you have learnt about Machine Learning, AI and Reinforcement Learning. You have also been briefed about some basic reinforcement learning terminology along with examples of learning environments. In this task you, will conduct a case study about one of the gym environments from here: https://gymnasium.farama.org/ . Navigate to the ‚ÄòEnvironments‚Äô section on the left side (see screenshot below), and pick any 1 environment within any of the categories (Classic Control, Box2D etc.,).  \n To complete this assignment, you may refer to the lecture material from Week 1.  Submission Details Submit a report (via OnTrack) discussing the following aspects for any chosen gym environment:  ‚Ä¢ Describe the states, actions, transition functions and rewards for it.  ‚Ä¢ Is it a discrete or continuous state environment? Justify. ‚Ä¢ Is it a discrete or continuous action environment? Justify. ‚Ä¢ Is the transition function deterministic or probabilistic? Why? ‚Ä¢ What would the optimal policy look like? Describe how the reward function enables the learning of such an optimal policy.  Constraints The submitted report should not exceed 600 words in length. Please make sure the report is well-organised, and maintains a high quality of writing with appropriate references included if required.",
  "SIT796-4.2D.pdf": "SIT796 Reinforcement Learning Distinction Task 4.2: Dynamic programming , Monte-Carlo Methods Overview During week 3, you have learnt about Dynamic Programming and MDPs, and in week 4, you learnt about on and off policy approaches.  (a) You are given a robotic arm shown in the figure above, for which you need to design a controller to do some task (say, picking up objects from a table). Which of the two approaches - (i) Dynamic programming and (ii) Monte-Carlo methods would you consider for this? What are their possible benefits and drawbacks? (b) As you are working with the robot, one of the components fail, and the robot is unable to move one of its joints. The other joints still work normally. Which of the above two controllers is more likely to adapt to the faulty robot without any change to the controller code? Why?  (c) Use the navigation environment from the week 2 workshop and find the optimal policy the using Monte-Carlo Control.  (d) Implementing (b) in a 100x100 environment could be challenging. Why? What steps could be taken to make the implementation easier? (You do not actually have to implement it in the 100x100 environment. Just provide answers.) (e) Consider an environment with discount factor \\gamma=0.9. Your associate is developing a new algorithm, and they report that after running several trajectories, the maximum and minimum possible instantaneous rewards achievable in this environment are 1 and -1., and that the most valuable state has a value ùëâ(ùë†)=23.2. You immediately tell them to recheck their algorithm. Why? Hint: Start from the fact that ùëâ(ùë†)=ùê∏[ùëü!+ùõæùëü\"+ùõæ#ùëü#+‚ãØ] Submission Details For this task, please submit screenshots of your implemented code. For the written components of this task, please provide clear explanations.  Constraints Keep your answers short and concise. Try not to exceed 600-800 words in total for the written components.",
  "SIT796-3.2C.pdf": "SIT796 Reinforcement Learning Credit Task 3.2: Exploration/Exploitation Dilemma and Choice of DP/RL  Overview During week 2, you were introduced to multiarmed bandits and action selection mechanisms. For this task, you may need to refer to lecture slides from week 2 and week 3, as well as the week 2 workshop code.   Submission Details 1. (a) Using the reinforcement learning code and MiniGrid-Empty-6x6-v0 environment provided in the workshop session in Week 2, implement a linearly decaying epsilon strategy (note that the original workshop code contained an exponentially decaying epsilon). That is, start with epsilon=1 and reduce it by 0.005 after each episode. Ensure that the epsilon value is at least 0.01 in every episode. Maintain all other hyperparameters unchanged. (b) Now run the code with a fixed epsilon value of i) 0.1 and ii) 0.9.  (c) In the same plot, using different colours, plot the reward curves for the linearly decaying epsilon strategy and compare it with the strategy of fixed epsilon values of 0.1 and 0.9. Explain what you observe from the three reward curves in the plot.  2. Consider the automated cleaning robot from task 2.1P. You are asked to develop a controller for this robot based on dynamic programming. What are the aspects you will consider before deciding whether dynamic programming is the right choice of algorithm for it?   Constraints Provide screenshots of the code and the plot. Keep the explanations short and to the point. Try not to exceed 500-600 words in total.",
  "SIT796-4.1P.pdf": "SIT796 Reinforcement Learning Pass Task 4.1: On/Off policy Overview 1. What is the difference between and on and off-policy algorithms? 2. Why was \\gamma set to 1 in the week 4 workshop?  Constraints Keep your answers short and concise. Try not to exceed 300-500 words in total for the written components.",
  "SIT796-6.1P.pdf": "SIT796 Reinforcement Learning Pass Task 6.1: Eligibility Traces  Overview During Week 6, you have learnt about Eligibility Traces and DYNA.   1. What are Eligibility Traces and what is their main advantage? 2. What is the main advantage of DYNA? What is its main disadvantage? Constraints Please keep your answers short and to the point.",
  "SIT796-2.1P.pdf": "SIT796 Reinforcement Learning Pass Task 2.1: AI powered cleaning robot and value computation  Overview 1. You have learnt about the differences between reinforcement learning, supervised learning and unsupervised learning. In this task, consider a futuristic AI-powered cleaning robot for household cleaning chores. Discuss features you would incorporate into it. Discuss each feature and describe whether it would use supervised, unsupervised or reinforcement learning to achieve it. Make sure you include at least 1 feature for each learning type (i.e., at least 1 for supervised, 1 for unsupervised and 1 for reinforcement learning). 2. For each of these figures, the red arrows indicate the policy ùúã. The optimal policy (which may or may not be indicated is denoted by ùúã‚àó. Assuming a discount factor of ùõæ=0.9, find the discounted and undiscounted values of state ‚Äòs‚Äô under the indicated policies. The reward for the treasure state is ùëü\"=1, and for every other state, it is ùëü#$%&'=‚àí0.1. a) In Fig 1, what are the discounted and undiscounted values ùëâ((ùë†) and ùëâ(‚àó(ùë†)? s        Fig 1. b) In Fig 2, what are the discounted and undiscounted values ùëâ((ùë†) and ùëâ(‚àó(ùë†)?  s        Fig 2.   Submission Details For 1., feel free to be creative with regards to your robot‚Äôs features. Aim to keep your submitted essay to at the most 500 words in length. For 2., along with the final values, please also show how you worked out your values. You may refer to slide 35 of week 1 lecture notes for some clues."
}