Deakin University CRICOS Provider Code: 00113B
Presented by: 
Dr. Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Monte Carlo Methods
Deakin University CRICOS Provider Code: 00113B
Monte Carlo Methods
2
Monte-carlo sampling:
The term Monte Carlo is used for any algorithm that is significantly random in its operation
Calculate 𝜋 by randomly placing a dot and counting frequency of dots inside or outside the circle.
Area of circular portion: 𝐴𝐶= 𝜋𝑟2/4
Area of square: 𝐴𝑠= 𝑟2
𝐴𝐶
𝐴𝑆
= 𝜋/4
𝜋 = 4 𝐴𝐶
𝐴𝑆
Deakin University CRICOS Provider Code: 00113B
Disadvantage of Dynamic Programming: requires a full and complete model of the MDP
Monte Carlo (MC) methods only require experience.
Monte Carlo Methods
3
In RL, MC methods simply need to sample: states, actions and rewards from the environment 
MC methods are incremental in an episode-by-episode sense. 
Episode: one attempt at solving the task (episodes end based on terminal condition)
In this sense they are similar to bandits – except stretched out over a number of steps (each acting like a separate bandit problem). 
Deakin University CRICOS Provider Code: 00113B
MC can be used to learn the state value function for a given policy – as we have seen this is referred to as 
prediction
•
Recall – The value of a state is the expected cumulative future discounted reward starting at that state (expected return). 
•
The simplest approach to learn this from experience is to average the observed returns after visiting the state. 
•
This is the notion that underlies MC.
•
The following example considers the case where we only calculate for the first-visit.
Monte Carlo (Prediction)
4
Forward: {𝑠0, 𝑎0, 𝑟0}, {𝑠1, 𝑎1, 𝑟1}, {𝑠𝑇, 𝑎𝑇, 𝑟𝑇}
Backward: {𝑠𝑇, 𝑎𝑇, 𝑟𝑇}, {𝑠1, 𝑎1, 𝑟1}, {𝑠0, 𝑎0, 𝑟0}
G=0
G= 𝑟0
G= 𝑟0 + 𝛾𝑟1
G= 𝑟0 + 𝛾𝑟1 + 𝛾2 𝑟𝑇
G=0
G= 𝛾∗0 + 𝑟𝑇
G= 𝛾∗𝑟𝑇+ 𝑟1
G= 𝛾2 𝑟𝑇+ 𝛾𝑟1+ 𝑟0
Deakin University CRICOS Provider Code: 00113B
First Visit Monte Carlo (example)
5
𝑠1
s2
s1
s4
𝑠𝑇
3 episodes, 𝛾= 1
𝑎1
𝑎2
𝑎3
𝑎4
r1 = 0
r2 = 0
r3 = 1
r4 = 2
𝐺1=0+0+1+2
s1
s6
s4
s1
𝑠𝑇
𝑎3
𝑎2
𝑎1
𝑎4
r1 = 0
r2 = 1
r3 = 0
r4 = 2
𝐺2=0+1+0+2
s1
s4
s1
𝑠𝑇
𝑎2
𝑎2
𝑎4
r1 = 0
r2 = −1
r3 = 2
𝐺3=0-1+2
𝑉(s1)=(3+3+1)/3=2.33
𝑉(s4)=(2+2+1)/3=1.66
Deakin University CRICOS Provider Code: 00113B
Every Visit Monte Carlo (example)
6
𝑠1
s2
s1
s4
𝑠𝑇
3 episodes, 𝛾= 1
𝑎1
𝑎2
𝑎3
𝑎4
r1 = 0
r2 = 0
r3 = 1
r4 = 2
s1
s6
s4
s1
𝑠𝑇
𝑎3
𝑎2
𝑎1
𝑎4
r1 = 0
r2 = 1
r3 = 0
r4 = 2
s1
s4
s1
𝑠𝑇
𝑎2
𝑎2
𝑎4
r1 = 0
r2 = −1
r3 = 2
𝑉(s1)      =    [  (0+0+1+2)+(1+2).    +.     (0+1+0+2)+(+2).      +.      (0-1+2)+(2).  ]/6
= 2.33
Deakin University CRICOS Provider Code: 00113B
If we compare a MC backup diagram against that of DP, below:
•
DP updates a single 𝑉(𝑠) for all actions based on the policy being followed. 
•
Whereas MC updates the 𝑉(𝑠) based only on the path followed by the policy.
–
Does not build upon other states estimates – no bootstrapping. 
Importantly the computational cost is independent of how many states there are. 
•
Particularly useful if we only want to calculate a subset of states. 
•
Learn only from states that occur
Monte Carlo (Prediction)
7
Deakin University CRICOS Provider Code: 00113B
A card game where you play against the dealer
•
Aim is to get a higher score than the dealer without going bust (> 21). 
•
You are both given two cards (one of the dealer's cards is initially hidden).
•
if you are dealt an ace and ten (21) and the dealer doesn’t have an ace or a ten then 
you win immediately
•
You can keep drawing more cards (called a hit) until you have as high as you want to risk. 
–
If you draw too many and go over 21 you have lost
•
Once you have drawn all the cards you desire you finish (called stick)
•
The dealer then reveals the hidden card
•
The dealer must keep drawing a card until they are ≥17.
–
If the dealer goes over 21 then you automatically win
•
Once the dealer finishes drawing cards, if they arent bust
–
you win if your score is higher
–
You draw (keep your money) if your sums are the same
–
Lose if your sum is less
•
Note an Ace (can be 1 or 11) allows us to take a risk (when 11) even when we have a good score
The difficulty is knowing when to draw based on what you have and the one card you can see the dealer has.
Example – Blackjack
8
https ://thumbor.forbes.com /thumbor/960x0/https%3A%2F%2Fspecials-im ages. forbesimg.com %2Fdam%2Fim ages erve%2F1051931270%2F960x0.jpg%3Ffit%3D scale
https ://www.blac kj ack.org/wp-content/uploads/2018/12/Blackjack-values-705x396.png
Deakin University CRICOS Provider Code: 00113B
Let’s consider a policy that we want to test 
•
The policy sticks if the player has a sum of 20 or 21
•
Otherwise it always hits
Example – Blackjack with MC Prediction
9
Sutton and Barto (2018)
To find the state value of this policy using 
MC prediction we can simulate many games 
using the policy
 
- average the returns observed.
Why does the estimated value function jump up for the last 
two rows in the rear? 
 
Why does it drop off for the whole last row on the left? 
 
Why are the frontmost values higher in the upper diagrams
than in the lower?
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Dr. Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Monte Carlo Control
Deakin University CRICOS Provider Code: 00113B
In DP we calculated state values because we can simply select the action that leads to the best next state. 
•
This is possible because we have a model of the environment – hence we can track what states come next and their value. 
•
However if we don’t have a model we can not do this. 
•
If we can estimate the  value of actions taken from a particular state (rather than just the value of the state), we can just choose the best actions based 
on the Q value. 
•
Often referred to as state-action values and denoted 𝑄𝜋(𝑠, 𝑎)
•
MC, with enough experience can estimate 𝑄∗
Monte Carlo with Action Values
11
- 
Some state-action pairs may never be visited. Eg if 𝜋 never uses an action we will never learn a value for it. 
 
 
This fits with the general problem we discussed with multi-armed bandits – maintaining exploration
 
In reality we do this by visiting state-action pairs multiple times to estimate the value. 
 
-         Therefore, MC finds the average of the returns from that state-action pair.
-
 Exploring starts – starting at a random state-action pair, where there is a probability of starting in any state-action pair
-
 Use a stochastic policy where all state-actions have a probability of being selected. 
Deakin University CRICOS Provider Code: 00113B
Recall, Control is a search for a policy 
•
whereas prediction was aiming to find the value – given a policy. 
Recall the GPI approach in DP
•
Recall, GPI maintains both a policy and an approximate value function.
–
The value function is updated to approach the value function of the current policy
–
The policy is improved based on the current value function
•
Hence these work against each other to some degree
 
In MC we can follow a similar approach
•
Again we can alternate between policy evaluation and policy iteration.
𝜋0 ՜
𝐸𝑞𝜋0 ՜
𝐼𝜋1 ՜
𝐸𝑞𝜋1 ՜
𝐼𝜋2 ՜
𝐸⋅⋅⋅՜
𝐼𝜋∗՜
𝐸𝑞∗
•
Except now we apply the evaluation based on the the complete trajectory.
•
Policy iteration is performed by selecting the greedy policy with respect to the current value function
•
This is also applied on the state-action values instead of state values and hence do not require a model
𝜋𝑠= arg max
𝑎
𝑞(𝑠, 𝑎)
Monte Carlo (Control)
12
Deakin University CRICOS Provider Code: 00113B
If we assume exploring starts and an infinite number of iterations
•
Then MC is guaranteed to converge using the following algorithm
Monte Carlo (Control): Exploring Starts (ES)
13
Deakin University CRICOS Provider Code: 00113B
Revisiting the Blackjack problem – it is now easy to 
fully solve using MC Control
•
Can easily arrange for exploring starts. 
•
Cycle through all the possible start positions and 
simulate what happens. 
•
Give all state-action values an initial value – say 0.
•
Apply the algorithm
The final strategy (left) is almost identical to the 
expert strategy of Thorp(1966)
Example – Solve Blackjack with MC Control
14
Sutton and Barto (2018)
Deakin University CRICOS Provider Code: 00113B
Exploring starts is a form of On-Policy Control
•
On-Policy – evaluates and improves the policy being followed when making decisions.
However, ES is fundamentally an unlikely assumption for most tasks. 
•
To avoid ES we need a way of visiting every state-action pair
•
Can be done using a stochastic policy. That is 𝜋𝑎𝑠> 0, for all s ∈𝒮, 𝑎∈ 𝒜𝑠
•
Then overtime shift this stochastic policy more towards a deterministic policy
•
In our discussion of multi-armed bandits we discussed some ways of doing this:
–
𝜀−𝑔𝑟𝑒𝑒𝑑𝑦, Upper-Confidence-Bound, soft-max. 
–
There are many others eg Thomson Sampling, we have not discussed.
Monte Carlo (Control): On-Policy without ES
15
If we assume 𝜀−𝑔𝑟𝑒𝑒𝑑𝑦 we say this is a stochastic policy because:
•
Non-greedy actions will have  Τ
𝜀
𝒜𝑠 probability of being selected
•
The greedy action will have 1 −𝜀+ Τ
𝜀
𝒜𝑠 probability of being selected.
•
Therefore, 𝜋𝑎𝑠≥Τ
𝜀
𝒜𝑠 for all s ∈𝒮, 𝑎∈ 𝒜𝑠, which aligns with the above condition – providing 𝜀> 0
•
Any of the above 𝜀−𝑠𝑜𝑓𝑡 policies also provide convergence guarantees as stochastic policies.
Greedy 
(best 
action)
any action
𝜀
1 −𝜀
But with probab. 
Τ
𝜀
𝒜𝑠 , this 
random action can 
be the best action.
Thus, total probability of best action:                
1 −𝜀+ Τ
𝜀
𝒜𝑠
Deakin University CRICOS Provider Code: 00113B
MC On-Policy Control is still GPI
•
Our 𝜀−𝑠𝑜𝑓𝑡 policy should move towards a greedy policy but as we can not actually achieve infinite trials we can not actually reach a fully greedy policy.
•
Hence our 𝜀−𝑠𝑜𝑓𝑡 policy should never actually reach greedy
Monte Carlo (Control): On-Policy without ES
16
Deakin University CRICOS Provider Code: 00113B
On-Policy evaluates and improves the policy being followed.
•
The issue is that the agent must perform non-optimal behaviour to learn about all state-actions
•
This means the state-action values being learnt is based on the current policy (which may not be 
optimal)
–
Hence, may learn significantly inferior values.
–
May think a state value is not-optimal (when it is) because the later actions were exploratory.
On/Off-Policy Learning
17
Off-Policy instead evaluates and improves the optimal (target) policy instead of the one followed. Allows the agent to 
explore without damaging the state-action values being learnt.
To do this you effectively maintain two policies
The current optimal policy being learnt – called the target policy
 
A behaviour policy that generates the decisions and can be more exploratory
 
In this way, the state-action values learn from data “off” the target policy, hence the approach is called off-policy learning
 
In this unit we will consider both on-policy and off-policy approaches to different algorithms
 
 
On-policy is generally simpler and is considered first. 
 
 
You can think of On-policy as simply having the same behaviour and target policies – hence is a special case of off-policy
 
 
Generally, on-policy learns faster, but off-policy is often more powerful
Deakin University CRICOS Provider Code: 00113B
Let’s suppose we have collected months of data about how your household uses its solar power cell. 
•
This data represents a policy – the behaviour followed by your house (Behaviour Policy)
Let’s say we want to learn the optimal policy so we can advise you where to save power
•
How do we do this when we only have data for your actual behaviour and not the optimal behaviour?
Hence, we want to learn a policy, 𝑣𝜋 or 𝑞𝜋, but only have a policy 𝑏, where 𝑏≠𝜋
This can be done using off-policy learning. 
•
However, it does require the assumption of coverage to be met: 
•
E.g. we require every action to be taken under 𝜋 must also be taken at least sometimes in 𝑏.
–
Hence, 𝜋𝑎𝑠> 0 ⇒𝑏𝑎𝑠> 0 
•
Therefore, our behaviour policy, 𝑏 should be at least to some degree stochastic. 
If our problem meets the assumption of coverage then we can learn the target policy 𝜋 from our observations of 𝑏 
using importance sampling.
Off-Policy Monte Carlo Prediction
18
Deakin University CRICOS Provider Code: 00113B
Importance Sampling allows us to estimate expected values under one distribution given samples from a separate 
distribution. 
•
In off-policy learning we weight the returns observed based on the relative probability of their future trajectories occurring under both the 
target and behaviour policies.
•
This is called the importance-sampling ratio
•
Given the start state 𝑆𝑡, we want to calculate the probability of the future path, 𝐴𝑡, 𝑆𝑡+1, 𝐴𝑡+1, …, 𝑆𝑇,occurring under any policy 𝜋 using
Importance Sampling 
19
𝑃𝑟𝐴𝑡, 𝑆𝑡+1, 𝐴𝑡+1, …, 𝑆𝑇𝑆𝑡, 𝐴𝑡:𝑇−1~𝜋
= 𝜋𝐴𝑡𝑆𝑡𝑝𝑆𝑡+1 𝑆𝑡, 𝐴𝑡𝜋𝐴𝑡+1 𝑆𝑡+1 ⋯𝑝𝑆𝑇𝑆𝑇−1, 𝐴𝑇−1
= ෑ
𝑘=𝑡
𝑇−1
𝜋𝐴𝑘𝑆𝑘𝑝𝑆𝑘+1 𝑆𝑘, 𝐴𝑘
𝜌𝑡:𝑇−1 = ς𝑘=𝑡
𝑇−1𝜋𝐴𝑘𝑆𝑘𝑝𝑆𝑘+1 𝑆𝑘, 𝐴𝑘
ς𝑘=𝑡
𝑇−1 𝑏𝐴𝑘𝑆𝑘𝑝𝑆𝑘+1 𝑆𝑘, 𝐴𝑘
= ς𝑘=𝑡
𝑇−1𝜋𝐴𝑘𝑆𝑘
ς𝑘=𝑡
𝑇−1𝑏𝐴𝑘𝑆𝑘
Note: while the transition probabilities of 
the MDP are unknown, because they 
are the same for the numerator and 
denominator they cancel
Where p is the state transition probability
Therefore, the relative probability or importance-sampling ratio is calculated as:
Deakin University CRICOS Provider Code: 00113B
Recall, we aim to find the state or state-action values (expected returns) of the target policy. 
•
However, the returns 𝐺𝑡 generated by the behaviour policy are incorrect 𝔼𝐺𝑡|𝑆𝑡= 𝑠= 𝑣𝑏(𝑠) and so can not be directly used to calculate 𝑣𝜋.
•
Instead, we use the importance-sampling ratio to transform the returns to the correct expected value.
𝔼𝜌𝑡:𝑇−1 𝐺𝑡|𝑆𝑡= 𝑠= 𝑣𝜋(𝑠)
•
E.g. simply multiply our observed return by the importance-sampling ratio. 
•
To calculate 𝑣𝜋(𝑠) we use:
𝑣𝜋𝑠=
σ𝑡∈𝒯(𝑠) 𝜌𝑡:𝑇(𝑡)−1 𝐺𝑡
𝒯(𝑠)
•
This equation has effectively joined each episode end to end (time steps continue across episode boundaries). 
•
This simple average of samplings is called an ordinary importance sampling
Ordinary Importance Sampling
20
The number of episodes a state s is visited at least once
The return after episode completes
Importance sample ratio for first termination following time t
Sum each first visit of s
Deakin University CRICOS Provider Code: 00113B
A powerful alternative is a weighted importance sampling using a weighted average.
𝑣𝜋𝑠=
σ𝑡∈𝒯(𝑠) 𝜌𝑡:𝑇(𝑡)−1 𝐺𝑡
σ𝑡∈𝒯(𝑠) 𝜌𝑡:𝑇(𝑡)−1
•
If the importance of the sample (denominator) is zero then 𝑣𝜋𝑠= 0.
•
In practice, the weighted importance sampling has lower variance and is preferred.
–
The ordinary importance sampling is easier to apply when we look at function approximation
Weighted Importance Sampling
21
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Dr. Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Monte Carlo: Off-policy Estimation
Deakin University CRICOS Provider Code: 00113B
Recall, when tracking our Q-value over time in our multi-armed bandit (Topic 4), we could maintain an incremental average. 
𝑄𝑛+1 = 𝑄𝑛+ 1
𝑛𝑅𝑛−𝑄𝑛
•
Therefore, we only needed to store the current 𝑄𝑛 and the value of 𝑛
We can perform a similar approach when using ordinary importance sampling except use the scaled return in place of the 
reward. E.g.
𝑉𝑛+1 = 𝑉𝑛+ 𝑊𝑖
𝑛
 𝐺𝑡−𝑉𝑛
•
Where 𝑊𝑖= 𝜌𝑡𝑖:𝑇(𝑡𝑖)−1, e.g. the weighting of a particular sample.
When using the weighted importance sampling however we need to perform a weighted average of the returns. 
•
Now we need to also track the cumulative sum 𝐶𝑛, where 𝐶𝑛+1 = 𝐶𝑛+ 𝑊𝑛+1, of the weights given and use this instead of 𝑛. E.g.
𝑉𝑛+1 = 𝑉𝑛+ 𝑊𝑛
𝐶𝑛
 𝐺𝑡−𝑉𝑛
•
Where 𝑊𝑛 is the weighting of the sample in the nth episode and 𝐶𝑛 is the cumulative weights up to and including the nth episode.
Incremental Implementation
23
Deakin University CRICOS Provider Code: 00113B
Off-Policy Monte Carlo Prediction
24
Deakin University CRICOS Provider Code: 00113B
Recall, On-policy Control evaluates and improves the policy being followed when making decisions. 
In Off-Policy the policy used to generate behaviour may be different to that is being evaluated as the target.
•
Advantage of this approach is target policy can be deterministic (greedy) while the behaviour policy can sample other actions.
Off-Policy MC Control can use one of the two methods discussed for Off-policy MC Prediction
•
As we require the behaviour policy to cover all possible 
state-actions that might be required by the target policy, 
we must have a nonzero probability of selecting all actions. 
–
This is required to insure the assumption of coverage.
–
To do this we must use an 𝜀−𝑠𝑜𝑓𝑡 behaviour policy
Off-Policy Monte Carlo Control
25
Deakin University CRICOS Provider Code: 00113B
This lecture focused on Monte Carlo methods for prediction and control.
•
Same capabilities as DP without a transition model! 
•
However, it needs full trajectories/episodes – TD learning will solve this 
issue. 
For more detailed information see Sutton and Barto (2018) 
Reinforcement Learning: An Introduction
•
Chapter 5: Monte Carlo Methods
•
http://incompleteideas.net/book/RLbook2020.pdf
Readings
26
