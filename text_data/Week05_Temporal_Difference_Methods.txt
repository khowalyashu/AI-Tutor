Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Recap
Deakin University CRICOS Provider Code: 00113B
Week 1
2
Sutton and Barto (2018)
Introduction & History, 
RL formulation: States, Actions, Rewards, Transition function, value function, 
action-value function 
Deakin University CRICOS Provider Code: 00113B
Week 2
3
Psychological aspects
Multiarmed Bandits
Deakin University CRICOS Provider Code: 00113B
Week 3
4
Types of MDPs
Dynamic Programming
Deakin University CRICOS Provider Code: 00113B
Week 4
5
Monte-Carlo Methods
Monte-Carlo Prediction
Monte-Carlo Control
(on/off policy â€“ 
importance sampling)
Deakin University CRICOS Provider Code: 00113B
Pros & Cons
6
DP
Monte-Carlo
Transition and reward 
model required
No model needed
Requires full sweep of the 
state/state-action space
Requires full trajectories 
of experience
Convergence guaranteed
Converges only if 
states/state-actions are 
visited enough number of 
times
In reality, we donâ€™t have perfect 
models, and we donâ€™t need full 
trajectories to learn
â€“ we learn on-the-go
Source: https://www.batterseachessclub.org.uk/10-benefits-of-playing-chess/
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Temporal Difference (TD) Learning
Deakin University CRICOS Provider Code: 00113B
A combination of Dynamic Programming and Monte Carlo methods:
â€¢ Like Monte Carlo it learns from experience and doesnâ€™t need a model
â€¢ Like Dynamic Programming it updates estimates using other learned estimates
Temporal Difference (TD) Learning
8
Sutton and Barto (2018)
Temporal difference
Related to time
Learning happens as time progresses - can update values without having to wait till 
the end of the episode
Deakin University CRICOS Provider Code: 00113B
Temporal Difference (TD) Learning
9
Sutton and Barto (2018)
In Monte-Carlo, we wait till the end of the episode and update 
values accordingly:
In TD, we take one step (and experience one reward) and still try to 
update the value:
This is the target
This is the target for TD
Deakin University CRICOS Provider Code: 00113B
Temporal Difference (TD) Learning
10
Sutton and Barto (2018)
The general name is TD(ğœ†). This just corresponds to ğœ†= 0
The TD target                          is a combination of both samples     
and an estimate (  ) 
Combination of MC 
sampling and 
bootstrapping
Deakin University CRICOS Provider Code: 00113B
TD â€“ error
11
Sutton and Barto (2018)
TD error ğ›¿
Depends on the  reward and next state
This is the error TD methods aim to minimise
1 step of experience + 
belief about the future
Belief about the 
goodness of the 
present state
Deakin University CRICOS Provider Code: 00113B
Clearly TD (Like MC) is better than DP if you do not have a model
â€¢
Most real world problems we do not have a complete or even a partial model â€“ making DP impossible
â€¢
DP is also highly computationally intensive.
TD is also obviously better over MC in online environments
â€¢
MC methods must wait until the end of the episode to be updated â€“ problematic in long or continuing tasks.
â€¢
If exploration has occurred, then many updates can not be made.
â€¢
Whereas, TD you can update more often (each step) â€“ doesnâ€™t matter how long the episode is. 
â€¢
TD is less susceptible to issues around exploratory actions â€“ only affects the step when the exploration was taken.
TD methods have been proven to converge when the policy is fixed.
â€¢
Shown for table-based methods in MDPs
But which is faster (MC or TD)?
TD vs MC vs DP
12
Deakin University CRICOS Provider Code: 00113B
A simple MDP where you start at C and move left or right until you hit a terminal state.
â€¢
Provided policy is just a 50/50 random split between left or right actions. 
â€¢
TD Prediction aims to learn the true value for each state
â€¢
In this example TD is always better than MC.
Example
13
Sutton and Barto (2018)
TD makes more efficient 
use of data
Deakin University CRICOS Provider Code: 00113B
TD Control
14
TD for Control
â€¢ On-Policy â€“ where our behaviour policy is the same policy we are learning 
(SARSA)
â€¢ Off-Policy â€“ where we have a separate behaviour policy from the target policy we 
are attempting to learn (Q learning)
Deakin University CRICOS Provider Code: 00113B
SARSA: On-Policy TD Control (2)
15
Convergence of SARSA is guaranteed if all state action pairs are guaranteed to be sampled an infinite 
number of times
Uses quintuple (ğ‘†ğ‘¡, ğ´ğ‘¡, ğ‘…ğ‘¡+1, ğ‘†ğ‘¡+1, ğ´ğ‘¡+1). Hence the name SARSA
Q-Learning: Off-Policy TD Control
The off-policy equivalent to SARSA is known as Q-Learning (Watkins, PhD thesis 1989). 
Notice the subtle difference to SARSA â€“ the learning target and the way the actions are selected is different
Deakin University CRICOS Provider Code: 00113B
SARSA vs Q-learning
17
Both SARSA and Q learning implemented with ğœ€-greedy exploration 
(ğœ€= 0.1)
Sutton and Barto (2018)
Note: if ğœ€ was reduced to 0 overtime then both algorithms will converge to 
an optimal â€œcliff edgeâ€ policy
Q learning target only â€œcaresâ€ about the best action 
However, SARSA is on-policy â€“ so it accounts for the action-selection and 
prefers the safer path
Even if there is some non-zero probability of falling into the cliff, Q learning still prefers the 
risky (but optimal) path
Deakin University CRICOS Provider Code: 00113B
Expected SARSA
18
Expected Sarsa updates its value based on the expected reward â€“ incorporating how likely an action is to 
be taken. 
â€¢
It constructs the learning target based on the probability of each action â€“ doesnâ€™t rely on maxQ(sâ€™,aâ€™) 
like Q learning, nor does it rely on the taken action.
â€¢
So it can act as either On-policy or Off-policy.
ğ‘„ğ‘†ğ‘¡, ğ´ğ‘¡â†ğ‘„ğ‘†ğ‘¡, ğ´ğ‘¡+ ğ›¼ğ‘…ğ‘¡+1 + ğ›¾ğ”¼ğœ‹ğ‘„ğ‘†ğ‘¡+1, ğ´ğ‘¡+1 |ğ‘†ğ‘¡+1 âˆ’ğ‘„ğ‘†ğ‘¡, ğ´ğ‘¡
ğ‘„ğ‘†ğ‘¡, ğ´ğ‘¡â†ğ‘„ğ‘†ğ‘¡, ğ´ğ‘¡+ ğ›¼ğ‘…ğ‘¡+1 + ğ›¾à·
ğ‘
ğœ‹ğ‘ğ‘†ğ‘¡+1 ğ‘„ğ‘†ğ‘¡+1, ğ‘âˆ’ğ‘„ğ‘†ğ‘¡, ğ´ğ‘¡
Deakin University CRICOS Provider Code: 00113B
Presented by: 
Thommen George Karimpanal
School of Information Technology
SIT796 Reinforcement Learning
Maximisation bias and double Q Learning
Deakin University CRICOS Provider Code: 00113B
Maximization bias
20
s
a0
a1
a2
a3
True Q values = 0
Q(s,a0)
Q(s,a1)
Q(s,a2)
Q(s,a2)
ğ‘šğ‘ğ‘¥ğ‘â€²ğ‘„ğ‘¡ğ‘Ÿğ‘¢ğ‘’(ğ‘ , ğ‘â€²) = 0
ğ‘šğ‘ğ‘¥ğ‘â€²ğ‘„ğ‘’ğ‘ ğ‘¡(ğ‘ , ğ‘â€²) > 0
This positive bias is called maximization bias
Deakin University CRICOS Provider Code: 00113B
Double Q-Learning
21
Proposed as a solution to biased learning
Maintain two values for each action (ğ‘„1(ğ‘) and ğ‘„2(ğ‘))
We then randomly choose one of these values to decide which action we are going to use to select the maximum action
But then update the value of the other ğ‘„ value for that action.
Can be proved that this addresses the maximization bias issue 
Deakin University CRICOS Provider Code: 00113B
Double Q-Learning
22
Such â€œdoubleâ€ versions also exist for SARSA and expected SARSA
Van Hasselt, Hado, Arthur Guez, and 
David Silver. "Deep reinforcement 
learning with double q-
learning." Proceedings of the AAAI 
conference on artificial intelligence. Vol. 
30. No. 1. 2016.
Deakin University CRICOS Provider Code: 00113B
Double Q-Learning
23
Deakin University CRICOS Provider Code: 00113B
n-step TD learning
24
The TD target relies partially on 
samples (experience) and 
partially on bootstrapping 
estimates 
So far, we only looked at 1 step 
of experience.  But in general, 
we can roll out multiple steps (n-
steps)
Deakin University CRICOS Provider Code: 00113B
This bootstrapping idea can easily be extended to On-Policy Control (and 
Expected Sarsa)
â€¢
Whereas, in ğ‘›-step TD we add all the rewards along with the final state estimate. In Sarsa 
we add all the rewards with the final state-action value estimate.
ğºğ‘¡:ğ‘¡+ğ‘›= ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + ğ›¾2ğ‘…ğ‘¡+3 + â‹¯+ ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘›+ ğ›¾ğ‘›ğ‘„ğ‘¡+ğ‘›âˆ’1 ğ‘†ğ‘¡+ğ‘›, ğ´ğ‘¡+ğ‘›
â€¢
In Expected Sarsa we do the same except in the last step we use a weighted sum of the 
estimates of possible actions.
ğºğ‘¡:ğ‘¡+ğ‘›= ğ‘…ğ‘¡+1 + ğ›¾ğ‘…ğ‘¡+2 + ğ›¾2ğ‘…ğ‘¡+3 + â‹¯+ ğ›¾ğ‘›âˆ’1ğ‘…ğ‘¡+ğ‘›+ ğ›¾ğ‘›à´¤ğ‘‰ğ‘¡+ğ‘›âˆ’1 ğ‘†ğ‘¡+ğ‘› 
â€¢
Where à´¤ğ‘‰ğ‘¡ğ‘  is the expected approximate value of state ğ‘ . 
â€¢
This is calculate using the estimated action values of all states from s weighted by the 
probability of their being selected using policy ğœ‹
à´¤ğ‘‰ğ‘¡ğ‘ = à·
ğ‘
ğœ‹ğ‘ğ‘ ğ‘„ğ‘¡(ğ‘ , ğ‘)
ğ’-step Sarsa (TD On-policy Control)
25
Sutton and Barto (2018)
Deakin University CRICOS Provider Code: 00113B
In Searching for a goal using on-policy control
â€¢
The path taken which are also all the states that will be backed up using MC
â€¢
The state-actions learnt using a one-step Sarsa and 10-step Sarsa
â€¢
It is evident that 10 step Sarsa will learn more state-action values. 
Example: Gridworld with ğ’-step Sarsa 
26
Sutton and Barto (2018)
Deakin University CRICOS Provider Code: 00113B
Putting it all together
27
Sutton and Barto (2018)
Deakin University CRICOS Provider Code: 00113B
This lecture focused on methods for solving MDPs using 
Temporal difference.
â€¢
Future topics will look into advanced topics in using Temporal 
difference learning. 
â€¢
Ensure you understand what was discussed here before doing the 
following topics
For more detailed information see Sutton and Barto (2018) 
Reinforcement Learning: An Introduction
â€¢
Chapter 6: Temporal-Difference Learning
â€¢
http://incompleteideas.net/book/RLbook2020.pdf
â€¢
Lecture content has been borrowed from the above mentioned 
book
Readings
28
